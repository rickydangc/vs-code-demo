{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import h3\n",
    "from tsnecuda import TSNE\n",
    "import math\n",
    "import json\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "##For merging sets of dictionaries\n",
    "from mergedeep import merge as dict_merge\n",
    "from mergedeep import  Strategy\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import geohash as gh\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(1, '../hve_challenger_train_sales_pipeline/hve_challenger_train_sale_dp/local_dependencies/')\n",
    "# from geoTree.utils import percentiles_list\n",
    "# from geoTree.geoTree import GeoTree\n",
    "\n",
    "##Modeling Related Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gpytorch\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import GatedResidualNetwork, VariableSelectionNetwork, GatedLinearUnit, AddNorm ,GateAddNorm\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler , OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight ,compute_sample_weight\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin , _OneToOneFeatureMixin \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "#from sklearn.manifold import TSNE\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "##Interpretability\n",
    "# import shap as shp\n",
    "#from tsnecuda import TSNE\n",
    "\n",
    "##Plotting Libs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "##Path to local dependencies\n",
    "#from torch_local.custom_layers import GMLPBlock, SegmentedMLP, self_attention\n",
    "#from torch_local.loss_functions import loss_pc10_adaptive\n",
    "\n",
    "##Model Tunning\n",
    "import ray\n",
    "from ray import tune,air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "from ray.air import session\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import (\n",
    "    LightningTrainer,\n",
    "    LightningConfigBuilder,\n",
    "    LightningCheckpoint,\n",
    ")\n",
    "import pickle\n",
    "import dill\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, pipeline,AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----device:cpu\n",
      "-----Pytorch version:1.12.1+cu102\n",
      "gpu available: False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"-----device:{}\".format(device))\n",
    "print(\"-----Pytorch version:{}\".format(torch.__version__))\n",
    "print(f\"gpu available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_list = [.01,.05,.1,.25,.5,.75,.9,.95,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set some global Vars and functions\n",
    "\n",
    "def pc10_calc(y, y_pred):    \n",
    "    return round(np.mean((np.abs(y-y_pred)/y)<=0.1),3)\n",
    "\n",
    "\n",
    "def pc10_accm(y, y_pred):    \n",
    "    return np.sum((np.abs(y-y_pred)/y)<=0.1)\n",
    "\n",
    "\n",
    "def pc10plus_accm_torch(y,y_pred):\n",
    "    ratio = y_pred/y\n",
    "    return torch.sum(ratio > 1.1)\n",
    "\n",
    "\n",
    "def pc10_accm_torch(y,y_pred):\n",
    "    diff = torch.abs(y-y_pred)/y\n",
    "    return torch.sum(diff<= 0.1)\n",
    "\n",
    "def median_bias(y, y_pred):\n",
    "    return round(((y_pred - y) / y).median(), 3)\n",
    "\n",
    "def pc10plus (y, y_pred):\n",
    "    return round(np.mean((y_pred/y) > 1.1), 3)\n",
    "\n",
    "def mape(y,y_pred):\n",
    "    diff = np.abs((y-y_pred)/y)\n",
    "    return round(np.mean(diff),3)\n",
    "\n",
    "# for dataframe\n",
    "def df_metric_wArg(df,colTrue,colPred):\n",
    "    return pd.Series({'count'      : df.shape[0],\n",
    "                      'pc10'      : pc10_calc(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'pc10+'     : pc10plus(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'median_bias': median_bias(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'mape' : mape(df.loc[:, colTrue], df.loc[:, colPred])\n",
    "                     })\n",
    "\n",
    "def adj_col_list(list_to_adj,ref_list,retain=True):\n",
    "    ##If True keep elements in ref_list\n",
    "    if retain:\n",
    "        result = [col for col in list_to_adj if col in ref_list]\n",
    "    else:\n",
    "        result = [col for col in list_to_adj if col not in ref_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the State\n",
    "state = ['WA']\n",
    "trgt_cnty = ['WA063']\n",
    "time = '2023m05'\n",
    "ple_size = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/challenger/data\n",
      "/home/jovyan/challenger/artifacts\n"
     ]
    }
   ],
   "source": [
    "datapath = f'/home/jovyan/challenger/data'\n",
    "artpath = f'/home/jovyan/challenger/artifacts'\n",
    "print(datapath)\n",
    "print(artpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 34.1 s, total: 1min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bluesky_data_train = pd.read_pickle(f'{datapath}/raw/bluesky_data_train_state_full_{time}_extended.pkl')\n",
    "bluesky_data_train=bluesky_data_train[bluesky_data_train['final_state'].isin(state)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_test =  pd.read_pickle(f'{datapath}/raw/bluesky_data_test_state_full_{time}_extended.pkl')\n",
    "bluesky_data_test=bluesky_data_test[bluesky_data_test['final_state'].isin(state)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_train['final_ntdtefnd'] = pd.to_datetime(bluesky_data_train['final_ntdtefnd'])\n",
    "bluesky_data_train['ntdtefnd_lag1'] = pd.to_datetime(bluesky_data_train['ntdtefnd_lag1'])\n",
    "\n",
    "bluesky_data_train['min_age'] = bluesky_data_train['final_ntdtefnd'].min()\n",
    "bluesky_data_train['sale_age_toanchr'] = ((bluesky_data_train.final_ntdtefnd.dt.year -bluesky_data_train.min_age.dt.year ) * 12 +  bluesky_data_train.final_ntdtefnd.dt.month - bluesky_data_train.min_age.dt.month).astype(int)   \n",
    "bluesky_data_train[\"sale_age_lag1\"] = ((bluesky_data_train.ntdtefnd_lag1.dt.year-bluesky_data_train.min_age.dt.year ) * 12 + (bluesky_data_train.ntdtefnd_lag1.dt.month - bluesky_data_train.min_age.dt.month)).astype(int,errors='ignore')\n",
    "\n",
    "bluesky_data_train['sale_qtr'] = bluesky_data_train.final_ntdtefnd.dt.quarter\n",
    "bluesky_data_train['log_trgt'] = np.log(bluesky_data_train.final_purprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sale_age_toanchr\n",
       "0      8742\n",
       "1      6377\n",
       "2      6729\n",
       "3      9107\n",
       "4     10712\n",
       "5     11244\n",
       "6     13204\n",
       "7     12173\n",
       "8     12509\n",
       "9     11730\n",
       "10    11332\n",
       "11    10149\n",
       "12     8933\n",
       "13     5861\n",
       "14     6715\n",
       "15    10027\n",
       "16    10105\n",
       "17    10707\n",
       "18    10839\n",
       "19     8517\n",
       "20     9428\n",
       "21     8453\n",
       "22     6950\n",
       "23     5393\n",
       "24     4573\n",
       "25     3939\n",
       "26     5144\n",
       "27     6712\n",
       "28     6610\n",
       "29     5823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bluesky_data_train.groupby('sale_age_toanchr').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_test['final_ntdtefnd'] = pd.to_datetime(bluesky_data_test['final_ntdtefnd'])\n",
    "bluesky_data_test['ntdtefnd_lag1'] = pd.to_datetime(bluesky_data_test['ntdtefnd_lag1'])\n",
    "\n",
    "bluesky_data_test['min_age'] = bluesky_data_train['final_ntdtefnd'].min()\n",
    "bluesky_data_test[\"sale_age_toanchr\"] = ((bluesky_data_test.final_ntdtefnd.dt.year-bluesky_data_test.min_age.dt.year ) * 12 + (bluesky_data_test.final_ntdtefnd.dt.month - bluesky_data_test.min_age.dt.month)).astype(int)\n",
    "bluesky_data_test[\"sale_age_lag1\"] = ((bluesky_data_test.ntdtefnd_lag1.dt.year-bluesky_data_test.min_age.dt.year ) * 12 + (bluesky_data_test.ntdtefnd_lag1.dt.month - bluesky_data_test.min_age.dt.month)).astype(int,errors='ignore')\n",
    "\n",
    "bluesky_data_test[\"sale_qtr\"] =   bluesky_data_test.final_ntdtefnd.dt.quarter\n",
    "bluesky_data_test['log_trgt'] = np.log(bluesky_data_test.final_purprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sale_age_toanchr\n",
       "30    8800\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bluesky_data_test.groupby('sale_age_toanchr').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_train['clmls_cnt_tot_btrm'] = pd.to_numeric(bluesky_data_train['clmls_cnt_tot_btrm'])\n",
    "bluesky_data_train['idmtax_nbr_lot_size_sq_ft'] = pd.to_numeric(bluesky_data_train['idmtax_nbr_lot_size_sq_ft'])\n",
    "bluesky_data_train['clmls_nbr_area_bsmt'] = pd.to_numeric(bluesky_data_train['clmls_nbr_area_bsmt'])\n",
    "\n",
    "bluesky_data_test['clmls_cnt_tot_btrm'] = pd.to_numeric(bluesky_data_test['clmls_cnt_tot_btrm'])\n",
    "bluesky_data_test['idmtax_nbr_lot_size_sq_ft'] = pd.to_numeric(bluesky_data_test['idmtax_nbr_lot_size_sq_ft'])\n",
    "bluesky_data_test['clmls_nbr_area_bsmt'] = pd.to_numeric(bluesky_data_test['clmls_nbr_area_bsmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_train['H8']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],8), axis=1)\n",
    "bluesky_data_train['H7']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],7), axis=1)\n",
    "bluesky_data_train['H6']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],6), axis=1)\n",
    "bluesky_data_train['H5']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],5), axis=1)\n",
    "bluesky_data_train['H4']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],4), axis=1)\n",
    "bluesky_data_train['H3']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],3), axis=1)\n",
    "bluesky_data_train['H2']=bluesky_data_train.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],2), axis=1)\n",
    "\n",
    "\n",
    "bluesky_data_test['H8']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],8), axis=1)\n",
    "bluesky_data_test['H7']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],7), axis=1)\n",
    "bluesky_data_test['H6']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],6), axis=1)\n",
    "bluesky_data_test['H5']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],5), axis=1)\n",
    "bluesky_data_test['H4']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],4), axis=1)\n",
    "bluesky_data_test['H3']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],3), axis=1)\n",
    "bluesky_data_test['H2']=bluesky_data_test.apply(lambda row: h3.geo_to_h3(row['lat'], row['long'],2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get different hash lengths\n",
    "bluesky_data_train['geohash4'] = bluesky_data_train.geohash5.apply(lambda x : x[:-1])\n",
    "bluesky_data_train['geohash3'] = bluesky_data_train.geohash5.apply(lambda x : x[:-2])\n",
    "bluesky_data_train['geohash2'] = bluesky_data_train.geohash5.apply(lambda x : x[:-3])\n",
    "bluesky_data_test['geohash4'] = bluesky_data_test.geohash5.apply(lambda x : x[:-1])\n",
    "bluesky_data_test['geohash3'] = bluesky_data_test.geohash5.apply(lambda x : x[:-2])\n",
    "bluesky_data_test['geohash2'] = bluesky_data_test.geohash5.apply(lambda x : x[:-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_train['quarter'] = bluesky_data_train.final_ntdtefnd.apply(lambda x: pd.Timestamp(x).quarter-1)\n",
    "bluesky_data_test['quarter'] = bluesky_data_test.final_ntdtefnd.apply(lambda x: pd.Timestamp(x).quarter-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence transformer to get house description embedding\n",
    "model = SentenceTransformer(\"/home/jovyan/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58min 56s, sys: 1min 17s, total: 1h 13s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clmls_desc_pub_rmrk_embed_test=bluesky_data_test.apply(lambda x: model.encode(x.clmls_desc_pub_rmrk,convert_to_tensor=True) if x.clmls_desc_pub_rmrk else torch.zeros([384]), axis=1)\n",
    "clmls_desc_pub_rmrk_embed_test=torch.stack([x for x in clmls_desc_pub_rmrk_embed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 3h 57min 57s, sys: 35min 59s, total: 1d 4h 33min 56s\n",
      "Wall time: 1h 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clmls_desc_pub_rmrk_embed_train=bluesky_data_train.apply(lambda x: model.encode(x.clmls_desc_pub_rmrk,convert_to_tensor=True) if x.clmls_desc_pub_rmrk else torch.zeros([384]), axis=1)\n",
    "clmls_desc_pub_rmrk_embed_train=torch.stack([x for x in clmls_desc_pub_rmrk_embed_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clmls_desc_pub_rmrk_embed_train,f'/home/jovyan/clmls_desc_pub_rmrk_embed_train_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clmls_desc_pub_rmrk_embed_test,f'/home/jovyan/clmls_desc_pub_rmrk_embed_test_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min, sys: 13 s, total: 10min 13s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_text_pub_lstg_cmnt_embed_test=bluesky_data_test.apply(lambda x: model.encode(x.ccmls_text_pub_lstg_cmnt,convert_to_tensor=True) if x.ccmls_text_pub_lstg_cmnt else torch.zeros([384]), axis=1)\n",
    "ccmls_text_pub_lstg_cmnt_embed_test=torch.stack([x for x in ccmls_text_pub_lstg_cmnt_embed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 4min 58s, sys: 5min 10s, total: 4h 10min 8s\n",
      "Wall time: 10min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_text_pub_lstg_cmnt_embed_train=bluesky_data_train.apply(lambda x: model.encode(x.ccmls_text_pub_lstg_cmnt,convert_to_tensor=True) if x.ccmls_text_pub_lstg_cmnt else torch.zeros([384]), axis=1)\n",
    "ccmls_text_pub_lstg_cmnt_embed_train=torch.stack([x for x in ccmls_text_pub_lstg_cmnt_embed_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_text_pub_lstg_cmnt_embed_train,f'/home/jovyan/ccmls_text_pub_lstg_cmnt_train_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_text_pub_lstg_cmnt_embed_test,f'/home/jovyan/ccmls_text_pub_lstg_cmnt_test_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 27min 23s, sys: 3min 14s, total: 2h 30min 38s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_desc_aplnc_embed_train=bluesky_data_train.apply(lambda x: model.encode(x.ccmls_desc_aplnc,convert_to_tensor=True) if x.ccmls_desc_aplnc else torch.zeros([384]), axis=1)\n",
    "ccmls_desc_aplnc_embed_train=torch.stack([x for x in ccmls_desc_aplnc_embed_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 6s, sys: 4.76 s, total: 4min 11s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_desc_aplnc_embed_test=bluesky_data_test.apply(lambda x: model.encode(x.ccmls_desc_aplnc,convert_to_tensor=True) if x.ccmls_desc_aplnc else torch.zeros([384]), axis=1)\n",
    "ccmls_desc_aplnc_embed_test=torch.stack([x for x in ccmls_desc_aplnc_embed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_desc_aplnc_embed_train,f'/home/jovyan/ccmls_desc_aplnc_train_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_desc_aplnc_embed_test,f'/home/jovyan/ccmls_desc_aplnc_test_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 5s, sys: 1min 1s, total: 46min 6s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_desc_eqpmnt_othr_embed_train=bluesky_data_train.apply(lambda x: model.encode(x.ccmls_desc_eqpmnt_othr,convert_to_tensor=True) if x.ccmls_desc_eqpmnt_othr else torch.zeros([384]), axis=1)\n",
    "ccmls_desc_eqpmnt_othr_embed_train=torch.stack([x for x in ccmls_desc_eqpmnt_othr_embed_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 1.07 s, total: 41.8 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ccmls_desc_eqpmnt_othr_embed_test=bluesky_data_test.apply(lambda x: model.encode(x.ccmls_desc_eqpmnt_othr,convert_to_tensor=True) if x.ccmls_desc_eqpmnt_othr else torch.zeros([384]), axis=1)\n",
    "ccmls_desc_eqpmnt_othr_embed_test=torch.stack([x for x in ccmls_desc_eqpmnt_othr_embed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_desc_aplnc_embed_train,f'/home/jovyan/ccmls_desc_eqpmnt_othr_train_{state[0]}_{time}_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ccmls_desc_aplnc_embed_test,f'/home/jovyan/ccmls_desc_eqpmnt_othr_test_{state[0]}_{time}_extended.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
