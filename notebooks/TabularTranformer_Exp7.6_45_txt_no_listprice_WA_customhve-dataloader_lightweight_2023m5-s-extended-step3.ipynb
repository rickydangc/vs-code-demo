{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import h3\n",
    "from tsnecuda import TSNE\n",
    "import math\n",
    "import json\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "##For merging sets of dictionaries\n",
    "from mergedeep import merge as dict_merge\n",
    "from mergedeep import  Strategy\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import geohash as gh\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(1, '../hve_challenger_train_sales_pipeline/hve_challenger_train_sale_dp/local_dependencies/')\n",
    "# from geoTree.utils import percentiles_list\n",
    "# from geoTree.geoTree import GeoTree\n",
    "\n",
    "##Modeling Related Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gpytorch\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import GatedResidualNetwork, VariableSelectionNetwork, GatedLinearUnit, AddNorm ,GateAddNorm\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler , OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight ,compute_sample_weight\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, _OneToOneFeatureMixin \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "#from sklearn.manifold import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "##Interpretability\n",
    "import shap as shp\n",
    "#from tsnecuda import TSNE\n",
    "\n",
    "##Plotting Libs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "##Path to local dependencies\n",
    "#from torch_local.custom_layers import GMLPBlock, SegmentedMLP, self_attention\n",
    "#from torch_local.loss_functions import loss_pc10_adaptive\n",
    "\n",
    "##Model Tunning\n",
    "import ray\n",
    "from ray import tune,air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "from ray.air import session\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import (\n",
    "    LightningTrainer,\n",
    "    LightningConfigBuilder,\n",
    "    LightningCheckpoint,\n",
    ")\n",
    "import pickle\n",
    "import dill\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, pipeline,AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----device:cpu\n",
      "-----Pytorch version:1.12.1+cu102\n",
      "gpu available: False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"-----device:{}\".format(device))\n",
    "print(\"-----Pytorch version:{}\".format(torch.__version__))\n",
    "print(f\"gpu available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_list = [.01,.05,.1,.25,.5,.75,.9,.95,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set some global Vars and functions\n",
    "\n",
    "def pc10_calc(y, y_pred):    \n",
    "    return round(np.mean((np.abs(y-y_pred)/y)<=0.1),3)\n",
    "\n",
    "\n",
    "def pc10_accm(y, y_pred):    \n",
    "    return np.sum((np.abs(y-y_pred)/y)<=0.1)\n",
    "\n",
    "\n",
    "def pc10plus_accm_torch(y,y_pred):\n",
    "    ratio = y_pred/y\n",
    "    return torch.sum(ratio > 1.1)\n",
    "\n",
    "\n",
    "def pc10_accm_torch(y,y_pred):\n",
    "    diff = torch.abs(y-y_pred)/y\n",
    "    return torch.sum(diff<= 0.1)\n",
    "\n",
    "def median_bias(y, y_pred):\n",
    "    return round(((y_pred - y) / y).median(), 3)\n",
    "\n",
    "def pc10plus (y, y_pred):\n",
    "    return round(np.mean((y_pred/y) > 1.1), 3)\n",
    "\n",
    "def mape(y,y_pred):\n",
    "    diff = np.abs((y-y_pred)/y)\n",
    "    return round(np.mean(diff),3)\n",
    "\n",
    "# for dataframe\n",
    "def df_metric_wArg(df,colTrue,colPred):\n",
    "    return pd.Series({'count'      : df.shape[0],\n",
    "                      'pc10'      : pc10_calc(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'pc10+'     : pc10plus(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'median_bias': median_bias(df.loc[:, colTrue], df.loc[:, colPred]),\n",
    "                      'mape' : mape(df.loc[:, colTrue], df.loc[:, colPred])\n",
    "                     })\n",
    "\n",
    "def adj_col_list(list_to_adj,ref_list,retain=True):\n",
    "    ##If True keep elements in ref_list\n",
    "    if retain:\n",
    "        result = [col for col in list_to_adj if col in ref_list]\n",
    "    else:\n",
    "        result = [col for col in list_to_adj if col not in ref_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the State\n",
    "state = ['WA']\n",
    "trgt_cnty = ['WA063']\n",
    "time = '2023m05'\n",
    "ple_size = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/challenger/data\n",
      "/home/jovyan/challenger/artifacts\n"
     ]
    }
   ],
   "source": [
    "datapath = f'/home/jovyan/challenger/data'\n",
    "artpath = f'/home/jovyan/challenger/artifacts'\n",
    "print(datapath)\n",
    "print(artpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.8 s, sys: 24.1 s, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bluesky_data_train = pd.read_pickle(f'{datapath}/raw/bluesky_data_train_state_full_{time}_extended.pkl')\n",
    "bluesky_data_train=bluesky_data_train[bluesky_data_train['final_state'].isin(state)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_test =  pd.read_pickle(f'{datapath}/raw/bluesky_data_test_state_full_{time}_extended.pkl')\n",
    "bluesky_data_test=bluesky_data_test[bluesky_data_test['final_state'].isin(state)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Loading Preprocessor**\n",
      "**Loading Preprocessor-Done**\n",
      "**Loading Training Preprocessed Data**\n",
      "**Loading Training Preprocessed Data-Done**\n",
      "**Loading Test Preprocessed Data**\n",
      "**Loading Test Preprocessed Data-Done**\n",
      "**Loading Input Dict**\n",
      "**Loading Input Dict-Done**\n",
      "CPU times: user 55.9 s, sys: 5.28 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##If data already created then load from disk \n",
    "#Load the preprocessor\n",
    "print('**Loading Preprocessor**')\n",
    "with open(f'{artpath}/{state[0]}_{time}_ple{ple_size}_processor_new_h3_txt_extended.dill', 'rb') as dill_file:\n",
    "    preprocessor=dill.load(dill_file)\n",
    "print('**Loading Preprocessor-Done**')\n",
    "\n",
    "#Load Preprocessed Train Data\n",
    "print('**Loading Training Preprocessed Data**')\n",
    "X_train_s= pd.read_pickle(f'{datapath}/preprocessed/{state[0]}_{time}_ple{ple_size}_X_train_s_new_h3_txt_extended.pkl').reset_index(drop=True)\n",
    "print('**Loading Training Preprocessed Data-Done**')\n",
    "\n",
    "#Load Preprocessed Test Data\n",
    "print('**Loading Test Preprocessed Data**')\n",
    "X_test_s= pd.read_pickle(f'{datapath}/preprocessed/{state[0]}_{time}_ple{ple_size}_X_test_s_new_state_h3_txt_extended.pkl').reset_index(drop=True)\n",
    "print('**Loading Test Preprocessed Data-Done**')\n",
    "\n",
    "#Load the input dict \n",
    "print('**Loading Input Dict**')\n",
    "with open(f'{artpath}/{state[0]}_{time}_ple{ple_size}_input_dict_new_h3_txt_extended.dill', 'rb') as dill_file:\n",
    "    input_dict=dill.load(dill_file)\n",
    "print('**Loading Input Dict-Done**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800, 153)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_s=X_test_s[['ccmls_cnt_bdrm', 'ccmls_cnt_btrm', 'ccmls_cnt_ppty_stry', 'ccmls_cnt_unit', 'ccmls_dt_blt_yr', 'ccmls_nbr_gla', 'clmls_cnt_bdrm', 'clmls_cnt_tot_btrm', 'clmls_cnt_bldng_stry_cl', 'clmls_cnt_unit', 'clmls_cnt_grge_car', 'clmls_dt_blt_yr', 'clmls_nbr_area_livng', 'clmls_cnt_prkng_spce_open','clmls_cnt_prkng_spce_tot', 'fre_cnt_unit', 'fatax_amt_tot_asesd_val', 'fatax_amt_asesd_imprmt_val', 'fatax_amt_asesd_lnd_val', 'fatax_cnt_bdrm', 'fatax_cnt_bldng_stry', 'fatax_cnt_prkng_spce', 'fatax_cnt_tot_btrm_hve', 'fatax_cnt_unit', 'fatax_dt_eff_blt_yr', 'fatax_nbr_livng_sq_ft', 'fatax_nbr_lot_size_sq_ft', 'fatax_amt_tot_mkt_val', 'fatax_dt_cnsttn_yr', 'fatax_dt_tax_yr', 'idmtax_amt_asesd_tot_val', 'idmtax_amt_asesd_imprmt_val', 'idmtax_amt_asesd_lnd_val', 'idmtax_cnt_bdrm', 'idmtax_cnt_bldng_stry_drvd', 'idmtax_cnt_grge_car', 'idmtax_cnt_tot_btrm_hve', 'idmtax_cnt_unit', 'idmtax_dt_cnsttn_yr', 'idmtax_nbr_lot_size_sq_ft', 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd', 'idmtax_amt_tot_mkt_val', 'idmtax_dt_tax_yr', 'lat', 'long', 'ucdp_cnt_prkng_spce', 'ucdp_cnt_tot_btrm_hve', 'ucdp_cnt_bdrm', 'ucdp_cnt_unit_hve', 'ucdp_effectiveage', 'ucdp_nbr_bldng_sq_ft_hve', 'ucdp_num_grge_hve', 'ucdp_quality_of_construction', 'ucdp_site_sqft', 'ucdp_stories', 'ucdp_year_built', 'ucdp_salescompsubjactualage', 'mortgage_rate_lag1', 'mortgage_rate_lag2', 'mortgage_rate_lag3', 'mortgage_rate_lag4', 'mortgage_rate_lag5', 'mortgage_rate_lag6', 'avg_price_3m_bdrm_geohash6_orig','avg_price_6m_bdrm_geohash6_orig','avg_price_12m_bdrm_geohash6_orig', 'purprice_lag1', 'csushpinsa_lag1', 'csushpinsa_lag2', 'csushpinsa_lag3', 'csushpinsa_lag4', 'csushpinsa_lag5', 'csushpinsa_lag6', 'ccmls_cnt_full_btrm', 'clmls_cnt_full_btrm', 'fatax_cnt_full_btrm', 'ucdp_cnt_full_btrm_hve', 'ccmls_cnt_half_btrm', 'clmls_cnt_half_btrm', 'ucdp_cnt_hlf_btrm_hve', 'ucdp_improvementsbasementarea', 'fatax_nbr_bsmt_sq_ft', 'ccmls_nbr_area_bsmt', 'clmls_nbr_area_bsmt', 'fatax_nbr_grge_sq_ft', 'clmls_amt_list_prce', 'ccmls_list_prce','clmls_desc_pub_rmrk', 'ccmls_text_pub_lstg_cmnt', 'ccmls_desc_aplnc', 'ccmls_desc_eqpmnt_othr','final_zip', 'final_state', 'final_statecounty', 'geohash6', 'geohash5', 'geohash4', 'geohash3', 'geohash2', 'H8', 'H7', 'H6', 'H5', 'H4', 'H3', 'H2', 'quarter', 'ccmls_type_ppty_hve', 'clmls_ppty_type', 'fnm_type_ppty_hve', 'fre_type_ppty_hve', 'idmtax_type_ppty_hve', 'ucdp_type_ppty_hve', 'clmls_type_cond', 'ccmls_type_cond', 'ucdp_condition_rating', 'idmtax_type_bldng_cnsttn_qual_hve', 'fatax_type_bldng_cnsttn_qual_hve', 'ucdp_qualityconstr_hve', 'fatax_cd_absentee_ownr', 'idmtax_cd_ownr_ocpd', 'fatax_type_bldng_styl', 'idmtax_type_bldng_styl', 'ccmls_type_home_styl', 'clmls_type_bldng_styl_cl', 'fatax_type_grge', 'idmtax_type_grge', 'ccmls_type_grge',  'fatax_type_bsmt', 'idmtax_type_bsmt', 'ccmls_type_bsmt', 'clmls_type_bsmt', 'ccmls_poor_cond_sale', 'clmls_poor_cond_sale', 'ccmls_renovated_sale', 'clmls_renovated_sale', 'ucdp_renovated_sale', 'ccmls_ind_new_cntrctn', 'clmls_ind_new_cntrctn', 'ucdp_ind_new_cntrctn', 'ccmls_sub_cond_sale', 'clmls_sub_cond_sale', 'final_address', 'ntdtefnd_lag1', 'final_ntdtefnd', 'clmls_dttm_list_orig_cl','sale_age_lag1', 'sale_age_toanchr', 'ccmls_dttm_mls_evnt', 'sale_age_toanchr_raw', 'final_statecounty_raw', 'final_state_raw', 'log_trgt']]\n",
    "X_train_s=X_train_s[['ccmls_cnt_bdrm', 'ccmls_cnt_btrm', 'ccmls_cnt_ppty_stry', 'ccmls_cnt_unit', 'ccmls_dt_blt_yr', 'ccmls_nbr_gla', 'clmls_cnt_bdrm', 'clmls_cnt_tot_btrm', 'clmls_cnt_bldng_stry_cl', 'clmls_cnt_unit', 'clmls_cnt_grge_car', 'clmls_dt_blt_yr', 'clmls_nbr_area_livng', 'clmls_cnt_prkng_spce_open','clmls_cnt_prkng_spce_tot', 'fre_cnt_unit', 'fatax_amt_tot_asesd_val', 'fatax_amt_asesd_imprmt_val', 'fatax_amt_asesd_lnd_val', 'fatax_cnt_bdrm', 'fatax_cnt_bldng_stry', 'fatax_cnt_prkng_spce', 'fatax_cnt_tot_btrm_hve', 'fatax_cnt_unit', 'fatax_dt_eff_blt_yr', 'fatax_nbr_livng_sq_ft', 'fatax_nbr_lot_size_sq_ft', 'fatax_amt_tot_mkt_val', 'fatax_dt_cnsttn_yr', 'fatax_dt_tax_yr', 'idmtax_amt_asesd_tot_val', 'idmtax_amt_asesd_imprmt_val', 'idmtax_amt_asesd_lnd_val', 'idmtax_cnt_bdrm', 'idmtax_cnt_bldng_stry_drvd', 'idmtax_cnt_grge_car', 'idmtax_cnt_tot_btrm_hve', 'idmtax_cnt_unit', 'idmtax_dt_cnsttn_yr', 'idmtax_nbr_lot_size_sq_ft', 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd', 'idmtax_amt_tot_mkt_val', 'idmtax_dt_tax_yr', 'lat', 'long', 'ucdp_cnt_prkng_spce', 'ucdp_cnt_tot_btrm_hve', 'ucdp_cnt_bdrm', 'ucdp_cnt_unit_hve', 'ucdp_effectiveage', 'ucdp_nbr_bldng_sq_ft_hve', 'ucdp_num_grge_hve', 'ucdp_quality_of_construction', 'ucdp_site_sqft', 'ucdp_stories', 'ucdp_year_built', 'ucdp_salescompsubjactualage', 'mortgage_rate_lag1', 'mortgage_rate_lag2', 'mortgage_rate_lag3', 'mortgage_rate_lag4', 'mortgage_rate_lag5', 'mortgage_rate_lag6', 'avg_price_3m_bdrm_geohash6_orig','avg_price_6m_bdrm_geohash6_orig','avg_price_12m_bdrm_geohash6_orig', 'purprice_lag1', 'csushpinsa_lag1', 'csushpinsa_lag2', 'csushpinsa_lag3', 'csushpinsa_lag4', 'csushpinsa_lag5', 'csushpinsa_lag6', 'ccmls_cnt_full_btrm', 'clmls_cnt_full_btrm', 'fatax_cnt_full_btrm', 'ucdp_cnt_full_btrm_hve', 'ccmls_cnt_half_btrm', 'clmls_cnt_half_btrm', 'ucdp_cnt_hlf_btrm_hve', 'ucdp_improvementsbasementarea', 'fatax_nbr_bsmt_sq_ft', 'ccmls_nbr_area_bsmt', 'clmls_nbr_area_bsmt', 'fatax_nbr_grge_sq_ft', 'clmls_amt_list_prce', 'ccmls_list_prce','clmls_desc_pub_rmrk', 'ccmls_text_pub_lstg_cmnt', 'ccmls_desc_aplnc', 'ccmls_desc_eqpmnt_othr','final_zip', 'final_state', 'final_statecounty', 'geohash6', 'geohash5', 'geohash4', 'geohash3', 'geohash2', 'H8', 'H7', 'H6', 'H5', 'H4', 'H3', 'H2', 'quarter', 'ccmls_type_ppty_hve', 'clmls_ppty_type', 'fnm_type_ppty_hve', 'fre_type_ppty_hve', 'idmtax_type_ppty_hve', 'ucdp_type_ppty_hve', 'clmls_type_cond', 'ccmls_type_cond', 'ucdp_condition_rating', 'idmtax_type_bldng_cnsttn_qual_hve', 'fatax_type_bldng_cnsttn_qual_hve', 'ucdp_qualityconstr_hve', 'fatax_cd_absentee_ownr', 'idmtax_cd_ownr_ocpd', 'fatax_type_bldng_styl', 'idmtax_type_bldng_styl', 'ccmls_type_home_styl', 'clmls_type_bldng_styl_cl', 'fatax_type_grge', 'idmtax_type_grge', 'ccmls_type_grge', 'fatax_type_bsmt', 'idmtax_type_bsmt', 'ccmls_type_bsmt', 'clmls_type_bsmt', 'ccmls_poor_cond_sale', 'clmls_poor_cond_sale', 'ccmls_renovated_sale', 'clmls_renovated_sale', 'ucdp_renovated_sale', 'ccmls_ind_new_cntrctn', 'clmls_ind_new_cntrctn', 'ucdp_ind_new_cntrctn', 'ccmls_sub_cond_sale', 'clmls_sub_cond_sale', 'final_address', 'ntdtefnd_lag1', 'final_ntdtefnd', 'clmls_dttm_list_orig_cl','sale_age_lag1', 'sale_age_toanchr', 'ccmls_dttm_mls_evnt', 'sale_age_toanchr_raw', 'final_statecounty_raw', 'final_state_raw', 'log_trgt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.20246045067208 0.6203030147821884\n"
     ]
    }
   ],
   "source": [
    "trgt_mean = preprocessor.named_transformers_['std_trgt'].mean_[0]\n",
    "trgt_s = preprocessor.named_transformers_['std_trgt'].scale_[0]\n",
    "print(trgt_mean,trgt_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cols_ple =preprocessor.transformers_[0][2]\n",
    "x_cols_other = adj_col_list(X_train_s.copy().columns.values.tolist(),x_cols_ple,retain=False)\n",
    "x_cols_other.remove('final_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define the data loaders - This is for training\n",
    "class MLP_Dataset_noleak(Dataset):\n",
    "    def __init__(self,X,trgt_col_name):#,state):\n",
    "        super(MLP_Dataset_noleak,self).__init__()\n",
    "        self.col_idx_dic = {}\n",
    "        for col in X.columns:\n",
    "             self.col_idx_dic[col] = X.columns.get_loc(col)\n",
    "        ##Define Model columns\n",
    "        self.trgt_col_name = trgt_col_name\n",
    "        # self.preprocsr = preprocsr\n",
    "        self.X_ple = np.array(X[x_cols_ple].copy().to_numpy().tolist(),dtype=float)\n",
    "        self.X_other = X[x_cols_other].copy().to_numpy().astype(float)\n",
    "        self.X_txt = np.array(X[['clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt','ccmls_desc_aplnc','ccmls_desc_eqpmnt_othr']].copy().to_numpy().tolist(),dtype=float)\n",
    "    def __len__(self):\n",
    "        return len(self.X_ple) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rows_ple = self.X_ple[idx]\n",
    "        rows_other = self.X_other[idx]\n",
    "        rows_txt = self.X_txt[idx]\n",
    "        rows_procs = {**{k: (rows_ple[v] if k in x_cols_ple else rows_other[v-len(x_cols_ple)-4]) for k,v in self.col_idx_dic.items() } ,  \n",
    "                      **{'CLS': -1},\n",
    "                      **{'clmls_desc_pub_rmrk': rows_txt[0]},\n",
    "                      **{'ccmls_text_pub_lstg_cmnt': rows_txt[1]},\n",
    "                      **{'ccmls_desc_aplnc': rows_txt[2]},\n",
    "                      **{'ccmls_desc_eqpmnt_othr': rows_txt[3]},\n",
    "                      }\n",
    "        rows_procs = {k: torch.squeeze(torch.tensor(np.array(v)).float(),dim=0) if k in x_cols_ple+['clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt','ccmls_desc_aplnc','ccmls_desc_eqpmnt_othr'] else torch.tensor(np.array(v)).unsqueeze(0).float() for k,v in rows_procs.items()}\n",
    "        target=rows_procs[self.trgt_col_name]\n",
    "        del rows_procs[self.trgt_col_name]\n",
    "        return rows_procs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Since data can be very large goal is to avoid creating copies of data.\n",
    "##Instead of having a data loader class we will use a function to return what we need\n",
    "def MLP_get_dataloader(dataset_fn,targtCol,df_train,df_test=None,split_train_frac=None,batchSize=500,numWorkers=0):\n",
    "    \n",
    "    test_loader = None\n",
    "    val_loader =None\n",
    "    \n",
    "    if df_test is not None:\n",
    "        dataset_test = dataset_fn(df_test,targtCol)\n",
    "        test_loader = DataLoader(dataset_test,batch_size=batchSize,num_workers=numWorkers,shuffle=False)\n",
    "    if split_train_frac is not None:\n",
    "        train_index = pd.Index(np.random.choice(df_train.index, int(df_train.shape[0]*split_train_frac),replace=False))\n",
    "        dataset_train = dataset_fn(df_train.iloc[train_index],targtCol)\n",
    "        dataset_val = dataset_fn(df_train.drop(train_index),targtCol)\n",
    "        val_loader= DataLoader(dataset_val,batch_size=batchSize,num_workers=numWorkers,shuffle=True)\n",
    "    else:\n",
    "        dataset_train = dataset_fn(df_train,targtCol)\n",
    "    train_loader= DataLoader(dataset_train,batch_size=batchSize,num_workers=numWorkers,shuffle=True)\n",
    "    return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = X_train_s.drop(columns=['final_address'])\n",
    "X_test_s = X_test_s.drop(columns=['final_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Instantiate the data loader objects\n",
    "x_cols_ple =preprocessor.transformers_[0][2]\n",
    "x_cols_other = adj_col_list(X_train_s.copy().columns.values.tolist(),x_cols_ple+['clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt','ccmls_desc_aplnc','ccmls_desc_eqpmnt_othr'],retain=False)\n",
    "remove_cols = ['ntdtefnd_lag1','final_ntdtefnd','clmls_dttm_list_orig_cl','ccmls_dttm_mls_evnt','sale_age_toanchr_raw','final_statecounty_raw','final_state_raw']#'final_address',\n",
    "for col in remove_cols:\n",
    "    x_cols_other.remove(col)\n",
    "    \n",
    "    \n",
    "##Instantiate the data loader objects\n",
    "train_loader_MLP,val_loader_MLP,test_loader_MLP=MLP_get_dataloader(dataset_fn = MLP_Dataset_noleak,\n",
    "                                                                   targtCol = 'log_trgt',\n",
    "                                                                    df_train = X_train_s.drop(columns=['ntdtefnd_lag1','final_ntdtefnd','clmls_dttm_list_orig_cl','ccmls_dttm_mls_evnt','sale_age_toanchr_raw','final_statecounty_raw','final_state_raw']).copy(),\n",
    "                                                                   df_test= X_test_s.drop(columns=['ntdtefnd_lag1','final_ntdtefnd','clmls_dttm_list_orig_cl','ccmls_dttm_mls_evnt','sale_age_toanchr_raw','final_statecounty_raw','final_state_raw']).copy(),\n",
    "                                                                   split_train_frac = 0.9,\n",
    "                                                                   batchSize = 700,\n",
    "                                                                   numWorkers=6\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Clear some memory \n",
    "del X_train_s\n",
    "del X_test_s\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd_dim_cal(cat_size):    \n",
    "    #For emedding size info https://ai.stackexchange.com/questions/28564/how-to-determine-the-embedding-size\n",
    "    if cat_size <= 100:\n",
    "        return int(min(50,cat_size/2))\n",
    "    elif 100 < cat_size <= 1000:\n",
    "        return int(min(500,cat_size/2))\n",
    "    else:\n",
    "        return int(75.6496 * np.log(cat_size + 176.623) - 41.4457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccmls_cnt_bdrm': 45,\n",
       " 'ccmls_cnt_btrm': 45,\n",
       " 'ccmls_cnt_ppty_stry': 45,\n",
       " 'ccmls_cnt_unit': 45,\n",
       " 'ccmls_dt_blt_yr': 45,\n",
       " 'ccmls_nbr_gla': 45,\n",
       " 'clmls_cnt_bdrm': 45,\n",
       " 'clmls_cnt_tot_btrm': 45,\n",
       " 'clmls_cnt_bldng_stry_cl': 45,\n",
       " 'clmls_cnt_unit': 45,\n",
       " 'clmls_cnt_grge_car': 45,\n",
       " 'clmls_dt_blt_yr': 45,\n",
       " 'clmls_nbr_area_livng': 45,\n",
       " 'clmls_cnt_prkng_spce_open': 45,\n",
       " 'clmls_cnt_prkng_spce_tot': 45,\n",
       " 'fre_cnt_unit': 45,\n",
       " 'fatax_amt_tot_asesd_val': 45,\n",
       " 'fatax_amt_asesd_imprmt_val': 45,\n",
       " 'fatax_amt_asesd_lnd_val': 45,\n",
       " 'fatax_cnt_bdrm': 45,\n",
       " 'fatax_cnt_bldng_stry': 45,\n",
       " 'fatax_cnt_prkng_spce': 45,\n",
       " 'fatax_cnt_tot_btrm_hve': 45,\n",
       " 'fatax_cnt_unit': 45,\n",
       " 'fatax_dt_eff_blt_yr': 45,\n",
       " 'fatax_nbr_livng_sq_ft': 45,\n",
       " 'fatax_nbr_lot_size_sq_ft': 45,\n",
       " 'fatax_amt_tot_mkt_val': 45,\n",
       " 'fatax_dt_cnsttn_yr': 45,\n",
       " 'fatax_dt_tax_yr': 45,\n",
       " 'idmtax_amt_asesd_tot_val': 45,\n",
       " 'idmtax_amt_asesd_imprmt_val': 45,\n",
       " 'idmtax_amt_asesd_lnd_val': 45,\n",
       " 'idmtax_cnt_bdrm': 45,\n",
       " 'idmtax_cnt_bldng_stry_drvd': 45,\n",
       " 'idmtax_cnt_grge_car': 45,\n",
       " 'idmtax_cnt_tot_btrm_hve': 45,\n",
       " 'idmtax_cnt_unit': 45,\n",
       " 'idmtax_dt_cnsttn_yr': 45,\n",
       " 'idmtax_nbr_lot_size_sq_ft': 45,\n",
       " 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd': 45,\n",
       " 'idmtax_amt_tot_mkt_val': 45,\n",
       " 'idmtax_dt_tax_yr': 45,\n",
       " 'lat': 45,\n",
       " 'long': 45,\n",
       " 'ucdp_cnt_prkng_spce': 45,\n",
       " 'ucdp_cnt_tot_btrm_hve': 45,\n",
       " 'ucdp_cnt_bdrm': 45,\n",
       " 'ucdp_cnt_unit_hve': 45,\n",
       " 'ucdp_effectiveage': 45,\n",
       " 'ucdp_nbr_bldng_sq_ft_hve': 45,\n",
       " 'ucdp_num_grge_hve': 45,\n",
       " 'ucdp_quality_of_construction': 45,\n",
       " 'ucdp_site_sqft': 45,\n",
       " 'ucdp_stories': 45,\n",
       " 'ucdp_year_built': 45,\n",
       " 'ucdp_salescompsubjactualage': 45,\n",
       " 'mortgage_rate_lag1': 45,\n",
       " 'mortgage_rate_lag2': 45,\n",
       " 'mortgage_rate_lag3': 45,\n",
       " 'mortgage_rate_lag4': 45,\n",
       " 'mortgage_rate_lag5': 45,\n",
       " 'mortgage_rate_lag6': 45,\n",
       " 'avg_price_3m_bdrm_geohash6_orig': 45,\n",
       " 'avg_price_6m_bdrm_geohash6_orig': 45,\n",
       " 'avg_price_12m_bdrm_geohash6_orig': 45,\n",
       " 'purprice_lag1': 45,\n",
       " 'csushpinsa_lag1': 45,\n",
       " 'csushpinsa_lag2': 45,\n",
       " 'csushpinsa_lag3': 45,\n",
       " 'csushpinsa_lag4': 45,\n",
       " 'csushpinsa_lag5': 45,\n",
       " 'csushpinsa_lag6': 45,\n",
       " 'ccmls_cnt_full_btrm': 45,\n",
       " 'clmls_cnt_full_btrm': 45,\n",
       " 'fatax_cnt_full_btrm': 45,\n",
       " 'ucdp_cnt_full_btrm_hve': 45,\n",
       " 'ccmls_cnt_half_btrm': 45,\n",
       " 'clmls_cnt_half_btrm': 45,\n",
       " 'ucdp_cnt_hlf_btrm_hve': 45,\n",
       " 'ucdp_improvementsbasementarea': 45,\n",
       " 'fatax_nbr_bsmt_sq_ft': 45,\n",
       " 'ccmls_nbr_area_bsmt': 45,\n",
       " 'clmls_nbr_area_bsmt': 45,\n",
       " 'fatax_nbr_grge_sq_ft': 45,\n",
       " 'clmls_amt_list_prce': 45,\n",
       " 'ccmls_list_prce': 45,\n",
       " 'final_zip': 568,\n",
       " 'final_state': 2,\n",
       " 'final_statecounty': 40,\n",
       " 'geohash6': 28363,\n",
       " 'geohash5': 3644,\n",
       " 'geohash4': 335,\n",
       " 'geohash3': 19,\n",
       " 'geohash2': 3,\n",
       " 'H8': 24554,\n",
       " 'H7': 7941,\n",
       " 'H6': 2290,\n",
       " 'H5': 602,\n",
       " 'H4': 135,\n",
       " 'H3': 27,\n",
       " 'H2': 7,\n",
       " 'quarter': 5,\n",
       " 'ccmls_type_ppty_hve': 3,\n",
       " 'clmls_ppty_type': 3,\n",
       " 'fnm_type_ppty_hve': 6,\n",
       " 'fre_type_ppty_hve': 5,\n",
       " 'idmtax_type_ppty_hve': 4,\n",
       " 'ucdp_type_ppty_hve': 3,\n",
       " 'clmls_type_cond': 16,\n",
       " 'ccmls_type_cond': 16,\n",
       " 'ucdp_condition_rating': 6,\n",
       " 'idmtax_type_bldng_cnsttn_qual_hve': 6,\n",
       " 'fatax_type_bldng_cnsttn_qual_hve': 7,\n",
       " 'ucdp_qualityconstr_hve': 79,\n",
       " 'fatax_cd_absentee_ownr': 6,\n",
       " 'idmtax_cd_ownr_ocpd': 3,\n",
       " 'fatax_type_bldng_styl': 43,\n",
       " 'idmtax_type_bldng_styl': 21,\n",
       " 'ccmls_type_home_styl': 37,\n",
       " 'clmls_type_bldng_styl_cl': 37,\n",
       " 'fatax_type_grge': 27,\n",
       " 'idmtax_type_grge': 14,\n",
       " 'ccmls_type_grge': 16,\n",
       " 'fatax_type_bsmt': 5,\n",
       " 'idmtax_type_bsmt': 10,\n",
       " 'ccmls_type_bsmt': 8,\n",
       " 'clmls_type_bsmt': 55,\n",
       " 'ccmls_poor_cond_sale': 3,\n",
       " 'clmls_poor_cond_sale': 3,\n",
       " 'ccmls_renovated_sale': 3,\n",
       " 'clmls_renovated_sale': 3,\n",
       " 'ucdp_renovated_sale': 3,\n",
       " 'ccmls_ind_new_cntrctn': 3,\n",
       " 'clmls_ind_new_cntrctn': 3,\n",
       " 'ucdp_ind_new_cntrctn': 3,\n",
       " 'ccmls_sub_cond_sale': 3,\n",
       " 'clmls_sub_cond_sale': 3,\n",
       " 'CLS': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict=dict()\n",
    "for k,v in input_dict.items():\n",
    "    output_dict[k]=emd_dim_cal(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccmls_cnt_bdrm': 22,\n",
       " 'ccmls_cnt_btrm': 22,\n",
       " 'ccmls_cnt_ppty_stry': 22,\n",
       " 'ccmls_cnt_unit': 22,\n",
       " 'ccmls_dt_blt_yr': 22,\n",
       " 'ccmls_nbr_gla': 22,\n",
       " 'clmls_cnt_bdrm': 22,\n",
       " 'clmls_cnt_tot_btrm': 22,\n",
       " 'clmls_cnt_bldng_stry_cl': 22,\n",
       " 'clmls_cnt_unit': 22,\n",
       " 'clmls_cnt_grge_car': 22,\n",
       " 'clmls_dt_blt_yr': 22,\n",
       " 'clmls_nbr_area_livng': 22,\n",
       " 'clmls_cnt_prkng_spce_open': 22,\n",
       " 'clmls_cnt_prkng_spce_tot': 22,\n",
       " 'fre_cnt_unit': 22,\n",
       " 'fatax_amt_tot_asesd_val': 22,\n",
       " 'fatax_amt_asesd_imprmt_val': 22,\n",
       " 'fatax_amt_asesd_lnd_val': 22,\n",
       " 'fatax_cnt_bdrm': 22,\n",
       " 'fatax_cnt_bldng_stry': 22,\n",
       " 'fatax_cnt_prkng_spce': 22,\n",
       " 'fatax_cnt_tot_btrm_hve': 22,\n",
       " 'fatax_cnt_unit': 22,\n",
       " 'fatax_dt_eff_blt_yr': 22,\n",
       " 'fatax_nbr_livng_sq_ft': 22,\n",
       " 'fatax_nbr_lot_size_sq_ft': 22,\n",
       " 'fatax_amt_tot_mkt_val': 22,\n",
       " 'fatax_dt_cnsttn_yr': 22,\n",
       " 'fatax_dt_tax_yr': 22,\n",
       " 'idmtax_amt_asesd_tot_val': 22,\n",
       " 'idmtax_amt_asesd_imprmt_val': 22,\n",
       " 'idmtax_amt_asesd_lnd_val': 22,\n",
       " 'idmtax_cnt_bdrm': 22,\n",
       " 'idmtax_cnt_bldng_stry_drvd': 22,\n",
       " 'idmtax_cnt_grge_car': 22,\n",
       " 'idmtax_cnt_tot_btrm_hve': 22,\n",
       " 'idmtax_cnt_unit': 22,\n",
       " 'idmtax_dt_cnsttn_yr': 22,\n",
       " 'idmtax_nbr_lot_size_sq_ft': 22,\n",
       " 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd': 22,\n",
       " 'idmtax_amt_tot_mkt_val': 22,\n",
       " 'idmtax_dt_tax_yr': 22,\n",
       " 'lat': 22,\n",
       " 'long': 22,\n",
       " 'ucdp_cnt_prkng_spce': 22,\n",
       " 'ucdp_cnt_tot_btrm_hve': 22,\n",
       " 'ucdp_cnt_bdrm': 22,\n",
       " 'ucdp_cnt_unit_hve': 22,\n",
       " 'ucdp_effectiveage': 22,\n",
       " 'ucdp_nbr_bldng_sq_ft_hve': 22,\n",
       " 'ucdp_num_grge_hve': 22,\n",
       " 'ucdp_quality_of_construction': 22,\n",
       " 'ucdp_site_sqft': 22,\n",
       " 'ucdp_stories': 22,\n",
       " 'ucdp_year_built': 22,\n",
       " 'ucdp_salescompsubjactualage': 22,\n",
       " 'mortgage_rate_lag1': 22,\n",
       " 'mortgage_rate_lag2': 22,\n",
       " 'mortgage_rate_lag3': 22,\n",
       " 'mortgage_rate_lag4': 22,\n",
       " 'mortgage_rate_lag5': 22,\n",
       " 'mortgage_rate_lag6': 22,\n",
       " 'avg_price_3m_bdrm_geohash6_orig': 22,\n",
       " 'avg_price_6m_bdrm_geohash6_orig': 22,\n",
       " 'avg_price_12m_bdrm_geohash6_orig': 22,\n",
       " 'purprice_lag1': 22,\n",
       " 'csushpinsa_lag1': 22,\n",
       " 'csushpinsa_lag2': 22,\n",
       " 'csushpinsa_lag3': 22,\n",
       " 'csushpinsa_lag4': 22,\n",
       " 'csushpinsa_lag5': 22,\n",
       " 'csushpinsa_lag6': 22,\n",
       " 'ccmls_cnt_full_btrm': 22,\n",
       " 'clmls_cnt_full_btrm': 22,\n",
       " 'fatax_cnt_full_btrm': 22,\n",
       " 'ucdp_cnt_full_btrm_hve': 22,\n",
       " 'ccmls_cnt_half_btrm': 22,\n",
       " 'clmls_cnt_half_btrm': 22,\n",
       " 'ucdp_cnt_hlf_btrm_hve': 22,\n",
       " 'ucdp_improvementsbasementarea': 22,\n",
       " 'fatax_nbr_bsmt_sq_ft': 22,\n",
       " 'ccmls_nbr_area_bsmt': 22,\n",
       " 'clmls_nbr_area_bsmt': 22,\n",
       " 'fatax_nbr_grge_sq_ft': 22,\n",
       " 'clmls_amt_list_prce': 22,\n",
       " 'ccmls_list_prce': 22,\n",
       " 'final_zip': 284,\n",
       " 'final_state': 1,\n",
       " 'final_statecounty': 20,\n",
       " 'geohash6': 734,\n",
       " 'geohash5': 582,\n",
       " 'geohash4': 167,\n",
       " 'geohash3': 9,\n",
       " 'geohash2': 1,\n",
       " 'H8': 723,\n",
       " 'H7': 639,\n",
       " 'H6': 549,\n",
       " 'H5': 301,\n",
       " 'H4': 67,\n",
       " 'H3': 13,\n",
       " 'H2': 3,\n",
       " 'quarter': 2,\n",
       " 'ccmls_type_ppty_hve': 1,\n",
       " 'clmls_ppty_type': 1,\n",
       " 'fnm_type_ppty_hve': 3,\n",
       " 'fre_type_ppty_hve': 2,\n",
       " 'idmtax_type_ppty_hve': 2,\n",
       " 'ucdp_type_ppty_hve': 1,\n",
       " 'clmls_type_cond': 8,\n",
       " 'ccmls_type_cond': 8,\n",
       " 'ucdp_condition_rating': 3,\n",
       " 'idmtax_type_bldng_cnsttn_qual_hve': 3,\n",
       " 'fatax_type_bldng_cnsttn_qual_hve': 3,\n",
       " 'ucdp_qualityconstr_hve': 39,\n",
       " 'fatax_cd_absentee_ownr': 3,\n",
       " 'idmtax_cd_ownr_ocpd': 1,\n",
       " 'fatax_type_bldng_styl': 21,\n",
       " 'idmtax_type_bldng_styl': 10,\n",
       " 'ccmls_type_home_styl': 18,\n",
       " 'clmls_type_bldng_styl_cl': 18,\n",
       " 'fatax_type_grge': 13,\n",
       " 'idmtax_type_grge': 7,\n",
       " 'ccmls_type_grge': 8,\n",
       " 'fatax_type_bsmt': 2,\n",
       " 'idmtax_type_bsmt': 5,\n",
       " 'ccmls_type_bsmt': 4,\n",
       " 'clmls_type_bsmt': 27,\n",
       " 'ccmls_poor_cond_sale': 1,\n",
       " 'clmls_poor_cond_sale': 1,\n",
       " 'ccmls_renovated_sale': 1,\n",
       " 'clmls_renovated_sale': 1,\n",
       " 'ucdp_renovated_sale': 1,\n",
       " 'ccmls_ind_new_cntrctn': 1,\n",
       " 'clmls_ind_new_cntrctn': 1,\n",
       " 'ucdp_ind_new_cntrctn': 1,\n",
       " 'ccmls_sub_cond_sale': 1,\n",
       " 'clmls_sub_cond_sale': 1,\n",
       " 'CLS': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.          0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.04639922  0.99892298  0.00215443  0.99999768]\n",
      " [ 0.90929743 -0.41614684  0.0926985   0.99569422  0.00430886  0.99999072]]\n"
     ]
    }
   ],
   "source": [
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    #n : User-defined scalar, set to 10,000 by the authors of Attention Is All You Need.\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n",
    " \n",
    "P = getPositionEncoding(seq_len=3, d=6, n=10000)\n",
    "print(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        #print(w.shape, t1.shape, b.shape)\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    #print(v1.shape)\n",
    "    return torch.cat([v1, v2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_vsn(torch.nn.Module):\n",
    "    def __init__(self,feat_num,in_dim,hidden_dim,out_dim,context_size=None, dropout=.1,norm=False):\n",
    "        super(custom_vsn,self).__init__()\n",
    "        self.feat_num = feat_num\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim =hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm\n",
    "        self.context_size = context_size   \n",
    "        self.softmax = torch.nn.Softmax(dim=1)    \n",
    "        ##layers for custom vsn\n",
    "        self.vsn_gate = GatedResidualNetwork(in_dim,hidden_dim,out_dim,dropout=.05,residual=True,context_size=context_size)\n",
    "        if self.norm:\n",
    "            self.vsn_GRN_norm = torch.nn.LayerNorm(out_dim, eps=1e-05, elementwise_affine=True)\n",
    "            \n",
    "    def forward(self, x_feats, context = None):\n",
    "        ##VSN  \n",
    "        x_vsn = torch.stack(x_feats,dim=1)\n",
    "\n",
    "        if self.context_size  is not None:\n",
    "                vsn_context = torch.tile(context.unsqueeze(1),(1,self.feat_num,1))\n",
    "                grn_weight = self.vsn_gate(x_vsn,context=vsn_context)\n",
    "        else:\n",
    "                grn_weight = self.vsn_gate(x_vsn)\n",
    "\n",
    "        grn_weight = self.softmax(grn_weight)\n",
    "        x_vsn = torch.sum(x_vsn*grn_weight,1)\n",
    "\n",
    "        if self.norm:\n",
    "                x_vsn = self.vsn_GRN_norm(x_vsn)\n",
    "        return x_vsn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x,get_attention=False):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        if get_attention:\n",
    "            return attention,weighted\n",
    "        else:\n",
    "            return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * torch.nn.functional.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols_embed_group = ['final_zip','final_statecounty', 'final_state',\n",
    "               'geohash','geohash6','geohash5','geohash4','geohash3','geohash2','H8','H7','H6','H5','H4','H3','H2','quarter',\n",
    "               'ccmls_type_ppty_hve','clmls_ppty_type','fnm_type_ppty_hve','fre_type_ppty_hve','idmtax_type_ppty_hve','ucdp_type_ppty_hve',\n",
    "               'clmls_type_cond','ccmls_type_cond','ucdp_condition_rating',\n",
    "               'idmtax_type_bldng_cnsttn_qual_hve','fatax_type_bldng_cnsttn_qual_hve', 'ucdp_qualityconstr_hve',\n",
    "               'fatax_cd_absentee_ownr', 'idmtax_cd_ownr_ocpd',\n",
    "               'fatax_type_bldng_styl', 'idmtax_type_bldng_styl', 'ccmls_type_home_styl', 'clmls_type_bldng_styl_cl',\n",
    "               'fatax_type_grge','idmtax_type_grge', 'ccmls_type_grge', 'clmls_type_grge',\n",
    "               'fatax_type_bsmt','idmtax_type_bsmt' , 'ccmls_type_bsmt','clmls_type_bsmt'] #\n",
    "x_cols_embed_no_group = ['ccmls_poor_cond_sale', 'clmls_poor_cond_sale', 'ccmls_renovated_sale','clmls_renovated_sale', 'ucdp_renovated_sale',\n",
    "               'ccmls_ind_new_cntrctn', 'clmls_ind_new_cntrctn', 'ucdp_ind_new_cntrctn', 'ccmls_sub_cond_sale','clmls_sub_cond_sale','clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "var_grps = dict()\n",
    "var_grps['beds'] = ['ucdp_cnt_bdrm', 'idmtax_cnt_bdrm', 'fatax_cnt_bdrm']#'ccmls_cnt_bdrm','clmls_cnt_bdrm',\n",
    "var_grps['full_bath']=['fatax_cnt_full_btrm','ucdp_cnt_full_btrm_hve']#'ccmls_cnt_full_btrm','clmls_cnt_full_btrm',\n",
    "var_grps['half_bath']=['ccmls_cnt_half_btrm','clmls_cnt_half_btrm','ucdp_cnt_hlf_btrm_hve']\n",
    "var_grps['basement']=['ucdp_improvementsbasementarea', 'fatax_nbr_bsmt_sq_ft']#,'ccmls_nbr_area_bsmt','clmls_nbr_area_bsmt'\n",
    "#var_grps['fire_place']=['ccmls_cnt_fp','clmls_cnt_fp','ucdp_cnt_fp_hve']\n",
    "var_grps['baths'] = ['ucdp_cnt_tot_btrm_hve', 'idmtax_cnt_tot_btrm_hve', 'fatax_cnt_tot_btrm_hve'] #'ccmls_cnt_btrm','clmls_cnt_tot_btrm',\n",
    "var_grps['sqft'] = ['ucdp_nbr_bldng_sq_ft_hve', 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd',  'fatax_nbr_livng_sq_ft']#'clmls_nbr_area_livng','ccmls_nbr_gla',\n",
    "var_grps['lotsize'] = ['fatax_nbr_lot_size_sq_ft', 'ucdp_site_sqft', 'idmtax_nbr_lot_size_sq_ft']\n",
    "var_grps['yr_blt'] = ['ucdp_year_built', 'idmtax_dt_cnsttn_yr', 'fatax_dt_eff_blt_yr']#'fatax_dt_cnsttn_yr','ccmls_dt_blt_yr','clmls_dt_blt_yr',\n",
    "var_grps['stories'] = [ 'idmtax_cnt_bldng_stry_drvd', 'ucdp_stories', 'fatax_cnt_bldng_stry'] #'ccmls_cnt_ppty_stry','clmls_cnt_bldng_stry_cl',\n",
    "var_grps['units'] = ['ucdp_cnt_unit_hve', 'idmtax_cnt_unit',  'fatax_cnt_unit','fre_cnt_unit'] #'ccmls_cnt_unit','clmls_cnt_unit', \n",
    "var_grps['ppty_type'] = ['ucdp_type_ppty_hve', 'idmtax_type_ppty_hve',  'fnm_type_ppty_hve', 'fre_type_ppty_hve'] #'clmls_ppty_type', 'ccmls_type_ppty_hve',\n",
    "var_grps['condition'] = ['ucdp_condition_rating'] #'ccmls_type_cond','clmls_type_cond',\n",
    "var_grps['parking'] = ['idmtax_cnt_grge_car', 'ucdp_num_grge_hve','fatax_cnt_prkng_spce', 'ucdp_cnt_prkng_spce']#'clmls_cnt_prkng_spce_tot','clmls_cnt_grge_car', 'clmls_cnt_prkng_spce_open'\n",
    "var_grps['garage_space']=['fatax_nbr_grge_sq_ft']\n",
    "var_grps['quality'] = ['idmtax_type_bldng_cnsttn_qual_hve','fatax_type_bldng_cnsttn_qual_hve', 'ucdp_qualityconstr_hve']\n",
    "var_grps['occupied']= ['idmtax_cd_ownr_ocpd','fatax_cd_absentee_ownr']\n",
    "var_grps['tax_values'] = ['fatax_amt_tot_asesd_val',  'idmtax_amt_asesd_tot_val']#'fatax_amt_calc_tot_val','idmtax_amt_tot_mkt_val','fatax_amt_tot_mkt_val'\n",
    "var_grps['land_values'] = ['fatax_amt_asesd_lnd_val','idmtax_amt_asesd_lnd_val']#'fatax_amt_calc_lnd_val','fatax_amt_mkt_lnd_val'\n",
    "var_grps['imprmt_values'] = ['fatax_amt_asesd_imprmt_val','idmtax_amt_asesd_imprmt_val']#'fatax_amt_calc_imprmt_val','fatax_amt_mkt_imprmt_val'\n",
    "var_grps['building_style']=['fatax_type_bldng_styl', 'idmtax_type_bldng_styl'] # 'ccmls_type_home_styl', 'clmls_type_bldng_styl_cl'\n",
    "var_grps['garage_type']=['fatax_type_grge','idmtax_type_grge']#'ccmls_type_grge','clmls_type_grge'\n",
    "var_grps['basement_type']=['fatax_type_bsmt','idmtax_type_bsmt']# ,'ccmls_type_bsmt','clmls_type_bsmt'\n",
    "#var_grps['doc_year']=['fatax_dt_doc_yr', 'fatax_dt_doc_yr_prr']\n",
    "#var_grps['tax_year']=['fatax_dt_tax_yr','idmtax_dt_tax_yr']\n",
    "#var_grps['sale_type']=['fatax_cd_sale','fre_type_sale_trans_hve']\n",
    "var_grps['list_price']=['clmls_amt_list_prce','ccmls_list_prce']\n",
    "#var_grps['total_room']=['clmls_cnt_tot_rm','fatax_cnt_tot_rm','idmtax_cnt_tot_rm','ucdp_cnt_tot_rm_hve']\n",
    "#var_grps['unemp_rate']=['unemployment_county_rate', 'unemployment_county_rate_mean12', 'unemployment_county_rate_mean6', 'unemployment_rate', 'unemployment_rate_mean12', 'unemployment_rate_mean6', 'unemployment_state_rate', 'unemployment_state_rate_mean12', 'unemployment_state_rate_mean6']\n",
    "#var_grps['other'] =  ['purprice_lag1']\n",
    "'''\n",
    "var_grps = dict()\n",
    "var_grps['beds'] = ['clmls_cnt_bdrm','ucdp_cnt_bdrm', 'idmtax_cnt_bdrm', 'ccmls_cnt_bdrm','fatax_cnt_bdrm']\n",
    "var_grps['full_bath']=['ccmls_cnt_full_btrm','clmls_cnt_full_btrm','fatax_cnt_full_btrm','ucdp_cnt_full_btrm_hve']\n",
    "var_grps['half_bath']=['ccmls_cnt_half_btrm','clmls_cnt_half_btrm','ucdp_cnt_hlf_btrm_hve']\n",
    "var_grps['basement']=['ucdp_improvementsbasementarea', 'fatax_nbr_bsmt_sq_ft', 'ccmls_nbr_area_bsmt','clmls_nbr_area_bsmt']\n",
    "#var_grps['fire_place']=['ccmls_cnt_fp','clmls_cnt_fp','ucdp_cnt_fp_hve']\n",
    "var_grps['baths'] = ['clmls_cnt_tot_btrm', 'ucdp_cnt_tot_btrm_hve', 'idmtax_cnt_tot_btrm_hve', 'ccmls_cnt_btrm', 'fatax_cnt_tot_btrm_hve']\n",
    "var_grps['sqft'] = ['clmls_nbr_area_livng', 'ucdp_nbr_bldng_sq_ft_hve', 'idmtax_nbr_unvrsl_bldng_sq_ft_drvd', 'ccmls_nbr_gla', 'fatax_nbr_livng_sq_ft']\n",
    "var_grps['lotsize'] = ['fatax_nbr_lot_size_sq_ft', 'ucdp_site_sqft', 'idmtax_nbr_lot_size_sq_ft']\n",
    "var_grps['yr_blt'] = ['ccmls_dt_blt_yr','ucdp_year_built', 'idmtax_dt_cnsttn_yr', 'fatax_dt_eff_blt_yr','clmls_dt_blt_yr','fatax_dt_cnsttn_yr']#\n",
    "var_grps['stories'] = [ 'ccmls_cnt_ppty_stry','idmtax_cnt_bldng_stry_drvd', 'ucdp_stories','clmls_cnt_bldng_stry_cl', 'fatax_cnt_bldng_stry'] #\n",
    "var_grps['units'] = ['ccmls_cnt_unit','clmls_cnt_unit', 'ucdp_cnt_unit_hve', 'idmtax_cnt_unit',  'fatax_cnt_unit','fre_cnt_unit'] #\n",
    "var_grps['ppty_type'] = ['clmls_ppty_type', 'ucdp_type_ppty_hve', 'idmtax_type_ppty_hve', 'ccmls_type_ppty_hve', 'fnm_type_ppty_hve', 'fre_type_ppty_hve']\n",
    "var_grps['condition'] = ['ccmls_type_cond','clmls_type_cond','ucdp_condition_rating'] #\n",
    "var_grps['parking'] = ['idmtax_cnt_grge_car', 'clmls_cnt_grge_car', 'ucdp_num_grge_hve','fatax_cnt_prkng_spce', 'ucdp_cnt_prkng_spce','clmls_cnt_prkng_spce_tot']#,'clmls_cnt_prkng_spce_open'\n",
    "var_grps['garage_space']=['fatax_nbr_grge_sq_ft']\n",
    "var_grps['quality'] = ['idmtax_type_bldng_cnsttn_qual_hve','fatax_type_bldng_cnsttn_qual_hve', 'ucdp_qualityconstr_hve']\n",
    "var_grps['occupied']= ['idmtax_cd_ownr_ocpd','fatax_cd_absentee_ownr']\n",
    "var_grps['tax_values'] = ['fatax_amt_tot_asesd_val',  'idmtax_amt_asesd_tot_val']#'fatax_amt_calc_tot_val','idmtax_amt_tot_mkt_val','fatax_amt_tot_mkt_val'\n",
    "var_grps['land_values'] = ['fatax_amt_asesd_lnd_val','idmtax_amt_asesd_lnd_val']#'fatax_amt_calc_lnd_val','fatax_amt_mkt_lnd_val'\n",
    "var_grps['imprmt_values'] = ['fatax_amt_asesd_imprmt_val','idmtax_amt_asesd_imprmt_val']#'fatax_amt_calc_imprmt_val','fatax_amt_mkt_imprmt_val'\n",
    "var_grps['building_style']=['fatax_type_bldng_styl', 'idmtax_type_bldng_styl', 'ccmls_type_home_styl', 'clmls_type_bldng_styl_cl']\n",
    "var_grps['garage_type']=['fatax_type_grge','idmtax_type_grge', 'ccmls_type_grge']#\n",
    "var_grps['basement_type']=['fatax_type_bsmt','idmtax_type_bsmt' , 'ccmls_type_bsmt','clmls_type_bsmt']\n",
    "#var_grps['doc_year']=['fatax_dt_doc_yr', 'fatax_dt_doc_yr_prr']\n",
    "#var_grps['tax_year']=['fatax_dt_tax_yr','idmtax_dt_tax_yr']\n",
    "#var_grps['sale_type']=['fatax_cd_sale','fre_type_sale_trans_hve']\n",
    "var_grps['list_price']=['clmls_amt_list_prce','ccmls_list_prce']\n",
    "#var_grps['total_room']=['clmls_cnt_tot_rm','fatax_cnt_tot_rm','idmtax_cnt_tot_rm','ucdp_cnt_tot_rm_hve']\n",
    "#var_grps['unemp_rate']=['unemployment_county_rate', 'unemployment_county_rate_mean12', 'unemployment_county_rate_mean6', 'unemployment_rate', 'unemployment_rate_mean12', 'unemployment_rate_mean6', 'unemployment_state_rate', 'unemployment_state_rate_mean12', 'unemployment_state_rate_mean6']\n",
    "#var_grps['other'] =  ['purprice_lag1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_group = [ 'final_zip','final_statecounty','lat','long','final_state','H7','H6','H5','H4','H3','H2','geohash6','geohash5','geohash4','geohash3','geohash2']#'geohash',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_group_vsn =['clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt']\n",
    "txt_group =['clmls_desc_pub_rmrk','ccmls_text_pub_lstg_cmnt','ccmls_desc_aplnc','ccmls_desc_eqpmnt_othr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_grps = dict()\n",
    "temporal_grps['avp']=['avg_price_12m_bdrm_geohash6_orig','avg_price_6m_bdrm_geohash6_orig','avg_price_3m_bdrm_geohash6_orig']#'avg_price_12m',\n",
    "temporal_grps['ushp']=[ 'csushpinsa_lag3', 'csushpinsa_lag2','csushpinsa_lag1']#,'csushpinsa','csushpinsa_lag6', 'csushpinsa_lag5', 'csushpinsa_lag4',\n",
    "#var_grps['house_growth_avg']='csushpinsa_mean12', 'csushpinsa_mean6']\n",
    "temporal_grps['mrtg_rate']=[ 'mortgage_rate_lag3', 'mortgage_rate_lag2', 'mortgage_rate_lag1']# 'mortgage_rate','mortgage_rate_lag6', 'mortgage_rate_lag5', 'mortgage_rate_lag4',\n",
    "#var_grps_time['mortage_rate_avg']=['mortgage_rate_mean12', 'mortgage_rate_mean6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_self_attention(nn.Module):\n",
    "    def __init__(self, input_dim,vdim=None, is_context=False, transformation=False):\n",
    "        super(custom_self_attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.vdim = vdim if vdim is not None else input_dim\n",
    "        self.is_context=is_context\n",
    "        if is_context:\n",
    "            self.query = GatedResidualNetwork(input_dim,input_dim,input_dim,residual=True,context_size=45)#torch.nn.Identity() #nn.Linear(input_dim, input_dim) # #\n",
    "            self.key = GatedResidualNetwork(input_dim,input_dim,input_dim,residual=True,context_size=45)#torch.nn.Identity() #nn.Linear(input_dim, input_dim)\n",
    "        else:\n",
    "            if transformation:\n",
    "                self.query = nn.Linear(self.input_dim, self.input_dim,bias=False)#torch.nn.Identity()\n",
    "                self.key = nn.Linear(self.input_dim, self.input_dim,bias=False)\n",
    "            else:\n",
    "                self.query =torch.nn.Identity()\n",
    "                self.key=torch.nn.Identity()\n",
    "        self.value = nn.Linear(self.vdim, self.vdim,bias=False)#torch.nn.Identity() #\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x, xk=None, xv=None, attn_mask=None,get_attention=True,context_k=None,context_q=None):\n",
    "        if self.is_context:\n",
    "            context_k=torch.tile(context_k.unsqueeze(1),(1,xk.shape[1],1)) if xk is not None else torch.tile(context_k.unsqueeze(1),(1,x.shape[1],1))\n",
    "            context_q=torch.tile(context_q.unsqueeze(1),(1,x.shape[1],1))\n",
    "            queries = self.query(x,context_q)\n",
    "            keys = self.key(xk,context_k) if xk is not None else self.key(x,context_k)\n",
    "        else:\n",
    "            queries = self.query(x)\n",
    "            keys = self.key(xk) if xk is not None else self.key(x)\n",
    "        values = xv if xv is not None else x #self.value(xv) if xv is not None else self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        \n",
    "        ##Note issue when all keys are masked , so implmented hack solution suggested\n",
    "        ## here https://github.com/pytorch/pytorch/issues/41508 to avoid nans in gradient\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            # L, S = x.size(-2), xk.size(-2) if xk is not None else x.size(-2)\n",
    "            # attn_bias = torch.zeros(L, S, dtype=x.dtype)\n",
    "            #print(attn_mask)\n",
    "            scores.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "            #print(attn_bias)\n",
    "            #attn_bias += attn_mask\n",
    "            #scores += attn_bias\n",
    "        #print(scores)    \n",
    "            scores = self.softmax(scores)\n",
    "\n",
    "            scores = scores.masked_fill(attn_mask,0)\n",
    "        else:\n",
    "            scores = self.softmax(scores)\n",
    "        #print(scores)\n",
    "        weighted = torch.bmm(scores, values)\n",
    "        if get_attention:\n",
    "            return weighted,scores\n",
    "        else:\n",
    "            return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAINT(torch.nn.Module):\n",
    "    def __init__(self,feat_num, feat_dim ,col_heads,cls_index = 0,return_cls=True,row_heads=None,kv=False,kv_feat_num=None,print_weight=False,mask=False,column_attn=False, is_context=False):\n",
    " \n",
    "        super(SAINT,self).__init__()\n",
    "        self.feat_num = feat_num\n",
    "        self.feat_dim = feat_dim\n",
    "        self.col_heads = col_heads\n",
    "        self.cls_index = cls_index\n",
    "        self.return_cls = return_cls\n",
    "        self.kv=kv\n",
    "        self.print_weight=print_weight\n",
    "        self.mask=mask\n",
    "        self.column_attn=column_attn\n",
    "        self.is_context=is_context\n",
    "        #self.kv_feat_num=kv_feat_num\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        #SAINT - https://arxiv.org/pdf/2106.01342.pdf\n",
    "        if is_context:\n",
    "            self.selfatt_feat =custom_self_attention(feat_dim, is_context=True,transformation=True)\n",
    "        else:\n",
    "            self.selfatt_feat =custom_self_attention(feat_dim,transformation=True)\n",
    "        self.selfatt_feat_norm = torch.nn.LayerNorm(feat_dim, eps=1e-05, elementwise_affine=True)\n",
    "        \n",
    "        self.feat_attn_fwd = torch.nn.Sequential(torch.nn.Linear(feat_dim,feat_dim),\n",
    "                                                 self.gelu,\n",
    "                                                 torch.nn.Linear(feat_dim,feat_dim),\n",
    "                                                 self.gelu)\n",
    "        self.selfatt_feat_norm2 = torch.nn.LayerNorm(feat_dim, eps=1e-05, elementwise_affine=True)\n",
    "        \n",
    "        ##Row attention \n",
    "        self.row_heads = row_heads if row_heads is not None else feat_num\n",
    "        if kv:\n",
    "            self.kv_transform=torch.nn.Linear(feat_dim*kv_feat_num,feat_dim*feat_num)\n",
    "        #self.rowatt = torch.nn.MultiheadAttention(feat_dim*feat_num,self.row_heads,batch_first=True)\n",
    "        self.rowatt = custom_self_attention(feat_dim*feat_num)\n",
    "        self.rowatt_norm = torch.nn.LayerNorm(feat_dim, eps=1e-05, elementwise_affine=True)\n",
    "        self.row_attn_fwd = torch.nn.Sequential(torch.nn.Linear(feat_dim,feat_dim),\n",
    "                                                self.gelu,\n",
    "                                                torch.nn.Linear(feat_dim,feat_dim),\n",
    "                                                self.gelu)\n",
    "        self.rowatt_norm2 = torch.nn.LayerNorm(feat_dim, eps=1e-05, elementwise_affine=True)        \n",
    "        \n",
    "    def forward(self,x,xk=None,xv=None,row_kv=None,mask_attn=None, context_k=None,context_q=None):\n",
    "        if self.is_context:\n",
    "            if xk is not None:\n",
    "                x_feat_attn, col_weights =  self.selfatt_feat(x,xk,xv,context_k=context_k,context_q=context_q)\n",
    "            else:\n",
    "                x_feat_attn, col_weights =  self.selfatt_feat(x,x,x,context_k=context_k,context_q=context_q) ##out shape is [seq_len, batch,featDIM = projSize]\n",
    "        else:\n",
    "            if xk is not None:\n",
    "                x_feat_attn, col_weights =  self.selfatt_feat(x,xk,xv)\n",
    "            else:\n",
    "                x_feat_attn, col_weights =  self.selfatt_feat(x,x,x)\n",
    "        x_feat_attn = self.selfatt_feat_norm(x_feat_attn)\n",
    "        x_feat_attn = x_feat_attn + x\n",
    "        x_feat_attn_fwd = self.feat_attn_fwd(x_feat_attn)\n",
    "        x_feat_attn_fwd = self.selfatt_feat_norm2(x_feat_attn_fwd)\n",
    "        x_feat_attn = x_feat_attn_fwd + x_feat_attn\n",
    "        if self.column_attn:\n",
    "            if self.return_cls:\n",
    "                x_feat_attn = x_feat_attn[:,self.cls_index,:]\n",
    "            return x_feat_attn\n",
    "        else:\n",
    "            ##Now apply the row attention\n",
    "            x_row = torch.flatten(x_feat_attn,start_dim=1).unsqueeze(0)\n",
    "            if self.kv:\n",
    "                row_kv_flat = self.kv_transform(torch.flatten(row_kv,start_dim=1).unsqueeze(0))\n",
    "                if self.mask:\n",
    "                    x_row_attn , row_weights = self.rowatt(x_row,row_kv_flat,row_kv_flat,attn_mask=mask_attn)\n",
    "                else:\n",
    "                    x_row_attn , row_weights = self.rowatt(x_row,row_kv_flat,row_kv_flat)\n",
    "            else:\n",
    "                if self.mask:\n",
    "                    x_row_attn , row_weights = self.rowatt(x_row,x_row,x_row,attn_mask=mask_attn)\n",
    "                else:\n",
    "                    x_row_attn , row_weights = self.rowatt(x_row,x_row,x_row)\n",
    "            if self.print_weight:\n",
    "                print(weights)\n",
    "            x_row_attn = x_row_attn.squeeze(0)\n",
    "            x_row_attn = x_row_attn.unflatten(1,(self.feat_num ,self.feat_dim))\n",
    "            x_row_attn = self.rowatt_norm(x_row_attn)\n",
    "            x_row_attn = x_row_attn + x_feat_attn\n",
    "            x_row_attn_fwd = self.row_attn_fwd(x_row_attn)\n",
    "            x_row_attn_fwd = self.rowatt_norm2(x_row_attn_fwd)\n",
    "            x_row_attn = x_row_attn_fwd + x_row_attn\n",
    "            ##only take the cls token part\n",
    "            if self.return_cls:\n",
    "                x_row_attn = x_row_attn[:,self.cls_index,:]\n",
    "            return x_row_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from paper Delving into Deep Imbalanced Regression (ICML 2021, Long Oral) https://github.com/YyzHarry/imbalanced-regression/tree/main\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from paper Delving into Deep Imbalanced Regression (ICML 2021, Long Oral) https://github.com/YyzHarry/imbalanced-regression/tree/main\n",
    "class FDS(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim, bucket_num=50, bucket_start=0, start_update=0, start_smooth=0,\n",
    "                 kernel='gaussian', ks=5, sigma=2, momentum=0.9):\n",
    "        super(FDS, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bucket_num = bucket_num\n",
    "        self.bucket_start = bucket_start\n",
    "        self.kernel_window = self._get_kernel_window(kernel, ks, sigma)\n",
    "        self.half_ks = (ks - 1) // 2\n",
    "        self.momentum = momentum\n",
    "        self.start_update = start_update\n",
    "        self.start_smooth = start_smooth\n",
    "\n",
    "        self.register_buffer('epoch', torch.zeros(1).fill_(start_update))\n",
    "        self.register_buffer('running_mean', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_var', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('smoothed_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('smoothed_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('num_samples_tracked', torch.zeros(bucket_num - bucket_start))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_kernel_window(kernel, ks, sigma):\n",
    "        assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "        half_ks = (ks - 1) // 2\n",
    "        if kernel == 'gaussian':\n",
    "            base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "            base_kernel = np.array(base_kernel, dtype=np.float32)\n",
    "            kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / sum(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "        elif kernel == 'triang':\n",
    "            kernel_window = triang(ks) / sum(triang(ks))\n",
    "        else:\n",
    "            laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "            kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / sum(\n",
    "                map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "        #logging.info(f'Using FDS: [{kernel.upper()}] ({ks}/{sigma})')\n",
    "        return torch.tensor(kernel_window, dtype=torch.float32)#.cuda()\n",
    "\n",
    "    def _get_bucket_idx(self, label):\n",
    "        label = np.float32(label)\n",
    "        _, bins_edges = np.histogram(a=np.array([], dtype=np.float32), bins=self.bucket_num, range=(-5, 5))\n",
    "        if label == max(label):\n",
    "            return self.bucket_num - 1\n",
    "        else:\n",
    "            return max(np.where(bins_edges > label)[0] - 1, self.bucket_start)\n",
    "\n",
    "    def _update_last_epoch_stats(self):\n",
    "        self.running_mean_last_epoch = self.running_mean\n",
    "        self.running_var_last_epoch = self.running_var\n",
    "\n",
    "        self.smoothed_mean_last_epoch = F.conv1d(\n",
    "            input=F.pad(self.running_mean_last_epoch.unsqueeze(1).permute(2, 1, 0),\n",
    "                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n",
    "            weight=self.kernel_window.view(1, 1, -1), padding=0\n",
    "        ).permute(2, 1, 0).squeeze(1)\n",
    "        self.smoothed_var_last_epoch = F.conv1d(\n",
    "            input=F.pad(self.running_var_last_epoch.unsqueeze(1).permute(2, 1, 0),\n",
    "                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n",
    "            weight=self.kernel_window.view(1, 1, -1), padding=0\n",
    "        ).permute(2, 1, 0).squeeze(1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.running_mean.zero_()\n",
    "        self.running_var.fill_(1)\n",
    "        self.running_mean_last_epoch.zero_()\n",
    "        self.running_var_last_epoch.fill_(1)\n",
    "        self.smoothed_mean_last_epoch.zero_()\n",
    "        self.smoothed_var_last_epoch.fill_(1)\n",
    "        self.num_samples_tracked.zero_()\n",
    "\n",
    "    def update_last_epoch_stats(self, epoch):\n",
    "        if epoch == self.epoch + 1:\n",
    "            self.epoch += 1\n",
    "            self._update_last_epoch_stats()\n",
    "            #logging.info(f\"Updated smoothed statistics of last epoch on Epoch [{epoch}]!\")\n",
    "\n",
    "    def update_running_stats(self, features, labels, epoch):\n",
    "        if epoch < self.epoch:\n",
    "            return\n",
    "\n",
    "        assert self.feature_dim == features.size(1), \"Input feature dimension is not aligned!\"\n",
    "        assert features.size(0) == labels.size(0), \"Dimensions of features and labels are not aligned!\"\n",
    "\n",
    "        buckets = np.array([self._get_bucket_idx(label) for label in labels])\n",
    "        for bucket in np.unique(buckets):\n",
    "            curr_feats = features[torch.tensor((buckets == bucket).astype(np.uint8))]\n",
    "            curr_num_sample = curr_feats.size(0)\n",
    "            curr_mean = torch.mean(curr_feats, 0)\n",
    "            curr_var = torch.var(curr_feats, 0, unbiased=True if curr_feats.size(0) != 1 else False)\n",
    "\n",
    "            self.num_samples_tracked[bucket - self.bucket_start] += curr_num_sample\n",
    "            factor = self.momentum if self.momentum is not None else \\\n",
    "                (1 - curr_num_sample / float(self.num_samples_tracked[bucket - self.bucket_start]))\n",
    "            factor = 0 if epoch == self.start_update else factor\n",
    "            self.running_mean[bucket - self.bucket_start] = \\\n",
    "                (1 - factor) * curr_mean + factor * self.running_mean[bucket - self.bucket_start]\n",
    "            self.running_var[bucket - self.bucket_start] = \\\n",
    "                (1 - factor) * curr_var + factor * self.running_var[bucket - self.bucket_start]\n",
    "\n",
    "        # make up for zero training samples buckets\n",
    "        for bucket in range(self.bucket_start, self.bucket_num):\n",
    "            if bucket not in np.unique(buckets):\n",
    "                if bucket == self.bucket_start:\n",
    "                    self.running_mean[0] = self.running_mean[1]\n",
    "                    self.running_var[0] = self.running_var[1]\n",
    "                elif bucket == self.bucket_num - 1:\n",
    "                    self.running_mean[bucket - self.bucket_start] = self.running_mean[bucket - self.bucket_start - 1]\n",
    "                    self.running_var[bucket - self.bucket_start] = self.running_var[bucket - self.bucket_start - 1]\n",
    "                else:\n",
    "                    self.running_mean[bucket - self.bucket_start] = (self.running_mean[bucket - self.bucket_start - 1] +\n",
    "                                                                     self.running_mean[bucket - self.bucket_start + 1]) / 2.\n",
    "                    self.running_var[bucket - self.bucket_start] = (self.running_var[bucket - self.bucket_start - 1] +\n",
    "                                                                    self.running_var[bucket - self.bucket_start + 1]) / 2.\n",
    "        #logging.info(f\"Updated running statistics with Epoch [{epoch}] features!\")\n",
    "\n",
    "    def smooth(self, features, labels, epoch):\n",
    "        if epoch < self.start_smooth:\n",
    "            return features\n",
    "\n",
    "        labels = labels.squeeze(1)\n",
    "        buckets = np.array([self._get_bucket_idx(label) for label in labels])\n",
    "        for bucket in np.unique(buckets):\n",
    "            features[torch.tensor((buckets == bucket).astype(np.uint8))] = calibrate_mean_var(\n",
    "                features[torch.tensor((buckets == bucket).astype(np.uint8))],\n",
    "                self.running_mean_last_epoch[bucket - self.bucket_start],\n",
    "                self.running_var_last_epoch[bucket - self.bucket_start],\n",
    "                self.smoothed_mean_last_epoch[bucket - self.bucket_start],\n",
    "                self.smoothed_var_last_epoch[bucket - self.bucket_start]\n",
    "            )\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class nn_vsn_saint(torch.nn.Module):\n",
    "    def __init__(self,config, target_dim,projSize,var_input_sizes,vsn_grps,temporal_grps,use_cuda=False,start_smooth=0):\n",
    "        \n",
    "        super(nn_vsn_saint,self).__init__()\n",
    "        \n",
    "        ##For debugging\n",
    "        self.print_switch = False\n",
    "        ###Initialization values##\n",
    "        self.target_dim = target_dim\n",
    "        self.projSize = projSize\n",
    "        self.var_input_sizes = var_input_sizes\n",
    "        self.vsn_grps = vsn_grps\n",
    "        self.temporal_grps = temporal_grps\n",
    "        self.use_cuda = use_cuda\n",
    "        self.min_embdd_size = 3\n",
    "        ##Activations\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.celu = torch.nn.CELU(alpha=6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.start_smooth=start_smooth\n",
    "        ##Initialize the embedding based layers\n",
    "        self.embed_layers = torch.nn.ModuleDict()\n",
    "        #print('**Embedding Cols***')\n",
    "        for k,v in var_input_sizes.items():\n",
    "            if k in x_cols_embed_group:                \n",
    "                self.embed_layers[k] =  torch.nn.Sequential(torch.nn.Embedding(v,output_dict[k]),\n",
    "                                                            torch.nn.Linear(output_dict[k],projSize,bias=False)#GatedResidualNetwork(output_dict[k],projSize,projSize,dropout=.05)\n",
    "                                                            )\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        self.dummy_layers = torch.nn.ModuleDict()\n",
    "        #print('**Dummy Cols***')\n",
    "        for k,v in var_input_sizes.items():\n",
    "            if k in x_cols_embed_no_group:                \n",
    "                self.dummy_layers[k] =  torch.nn.Embedding(v,output_dict[k])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        ##Initialize  the linear projections\n",
    "        self.proj_layers = torch.nn.ModuleDict()\n",
    "        for k,v in var_input_sizes.items():\n",
    "            if k  in x_cols_embed_group:\n",
    "                self.proj_layers[k] = torch.nn.Identity()\n",
    "            elif k  in x_cols_embed_no_group:\n",
    "                continue\n",
    "            elif k  in txt_group:\n",
    "                continue\n",
    "            #elif k  in txt_group_appr:\n",
    "                #continue\n",
    "            elif k == 'CLS':\n",
    "                self.proj_layers[k] = torch.nn.Linear(v,projSize)\n",
    "                self.proj_layers[k+'_avp'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "                self.proj_layers[k+'_ushp'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "                self.proj_layers[k+'_mrtg_rate'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "                self.proj_layers[k+'_spatial'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "                self.proj_layers[k+'_txt'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "                self.proj_layers[k+'_txt_appr'] = torch.nn.Linear(v,projSize,bias=False)\n",
    "            elif k not in ['sale_age_toanchr','sale_age_lag1','purprice_lag1','H8','geohash']:\n",
    "                self.proj_layers[k]  = torch.nn.Linear(v,projSize,bias=False) #GatedResidualNetwork(v,projSize,projSize,dropout=.05,residual=True)#\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "             \n",
    "        self.temporal_layer_attn = torch.nn.ModuleDict()\n",
    "        self.temporal_layer_norm = torch.nn.ModuleDict()\n",
    "        for k,v in temporal_grps.items():\n",
    "                self.temporal_layer_attn[k] = torch.nn.MultiheadAttention(projSize,15,batch_first=True)\n",
    "                self.temporal_layer_norm[k] = torch.nn.LayerNorm(projSize, eps=1e-05, elementwise_affine=True)                  \n",
    "            \n",
    "        ##Initialize the VSN blocks\n",
    "        self.vsn_layer = torch.nn.ModuleDict()\n",
    "        for k,v in vsn_grps.items():\n",
    "            if k == 'other':\n",
    "                continue\n",
    "            else:\n",
    "                self.vsn_layer[k] = custom_vsn(len(v),projSize,projSize,projSize,dropout=config['vsn_dp'],norm=True, context_size=projSize)\n",
    "                \n",
    "        self.spatial_time_layer = torch.nn.ModuleDict()\n",
    "        for k in spatial_group:\n",
    "            self.spatial_time_layer[k] = GatedResidualNetwork(projSize,projSize,projSize,dropout=config['spatial_time_dp'],residual=True,context_size=45)#torch.nn.Identity() #\n",
    "        \n",
    "        self.txt_layer = torch.nn.ModuleDict()\n",
    "        for k in txt_group:\n",
    "            self.txt_layer[k] = torch.nn.Linear(384,projSize,bias=False)\n",
    "            \n",
    "        #self.txt_layer_appr = torch.nn.ModuleDict()\n",
    "        #for k in txt_group_appr:\n",
    "            #self.txt_layer_appr[k] = torch.nn.Linear(384,projSize,bias=False)\n",
    "                                                    \n",
    "        self.vsn_layer_txt = custom_vsn(2,projSize,projSize,projSize,dropout=config['vsn_dp'],norm=True)\n",
    "        \n",
    "        ##Projections and transformations related to time\n",
    "        self.proj_time = SineActivation(1, projSize)#GatedResidualNetwork(var_input_sizes['sale_age_toanchr'],var_input_sizes['sale_age_toanchr'],var_input_sizes['sale_age_toanchr'],dropout=.0, residual=True)\n",
    "\n",
    "        self.proj_time_lag1 = SineActivation(1, projSize)#GatedResidualNetwork(var_input_sizes['sale_age_lag1'],var_input_sizes['sale_age_lag1'],var_input_sizes['sale_age_lag1'],dropout=.0, residual=True)\n",
    "        self.proj_dummy  = GatedResidualNetwork(10,projSize,projSize,dropout=config['dummy_dp']) \n",
    "        \n",
    "        self.proj_prior_sale = GatedResidualNetwork(projSize,projSize,projSize,dropout=config['prior_sale_dp'],context_size=projSize) \n",
    "        \n",
    "        self.saint_spatial=SAINT(len(spatial_group)+1, projSize ,3,cls_index=0,return_cls=True,row_heads=None) #config['heads_spatial']\n",
    "        self.proj_spatial=GatedResidualNetwork(projSize,projSize,projSize,dropout=0.1,context_size=projSize)\n",
    "        \n",
    "        #self.saint_txt_appr=SAINT(2, projSize, 2,cls_index=0,return_cls=True,row_heads=None,column_attn=True)\n",
    "        self.saint_txt=SAINT(4, projSize, 4,cls_index=0,return_cls=True,row_heads=None,is_context=True,column_attn=True)\n",
    "        self.saint_txt_inter=SAINT(4, projSize, 4,cls_index=0,return_cls=True,is_context=True,row_heads=None)\n",
    "        self.saint_feat=SAINT(27, projSize,15,cls_index=0,return_cls=True,row_heads=27,is_context=True)\n",
    "        self.saint_feat_inter=SAINT(27, projSize,15,cls_index=0,return_cls=True,row_heads=27,is_context=True)\n",
    "        self.saint_purprice_lag=SAINT(4, projSize,15,cls_index=0,return_cls=True,row_heads=15,kv=True,kv_feat_num=1)\n",
    "        self.conv= nn.Conv1d(4,4,2, stride=1)\n",
    "        self.avg_p_cv = nn.AvgPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        #self.bn=torch.nn.BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True)\n",
    "        \n",
    "        self.proj_feat  = GatedResidualNetwork(172,172,172,dropout=config['proj_dp'],residual=True) #GatedResidualNetwork(3*projSize,3*projSize,3*projSize,dropout=0,context_size=var_input_sizes['sale_age_toanchr'],residual=True)\n",
    "        \n",
    "        self.FDS = FDS(\n",
    "                feature_dim=217, bucket_num=50, bucket_start=0,\n",
    "                start_update=0, start_smooth=self.start_smooth, kernel='gaussian', ks=3, sigma=2, momentum=0.5\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.mlp_feat = torch.nn.Linear(217,target_dim,bias=True)#3*projSize\n",
    "        #self.out_mlp_sigma = torch.nn.Linear(3*projSize+21,target_dim,bias=True)\n",
    "        \n",
    "    def forward(self,inputs,targets=None,epoch=None,fsd=False):\n",
    "        \n",
    "        targets=targets.unsqueeze(-1)\n",
    "        ##Process the temporal blocks\n",
    "        xt = self.proj_time(inputs['sale_age_toanchr'])#inputs['sale_age_toanchr']#\n",
    "        inputs['sale_age_lag1'] =torch.nan_to_num(inputs['sale_age_lag1'])\n",
    "        xt_lag1 =self.proj_time_lag1(inputs['sale_age_lag1'])#inputs['sale_age_lag1']#\n",
    "        ##Process the embedding layers\n",
    "        x_emdd = {}\n",
    "        x_txt = []\n",
    "        #x_txt_appr = []\n",
    "        for k,v in inputs.items() :\n",
    "            if k in self.embed_layers.keys():             \n",
    "                x_emdd[k] = torch.squeeze(self.embed_layers[k](v.int()),dim=1)\n",
    "            elif k in self.txt_layer.keys():\n",
    "                x_txt.append(self.txt_layer[k](v))\n",
    "            #elif k in self.txt_layer_appr.keys():\n",
    "                #x_txt_appr.append(self.txt_layer_appr[k](v))\n",
    "            else:\n",
    "                continue\n",
    "        x_dummy = {}\n",
    "        for k,v in inputs.items() :\n",
    "            if k in self.dummy_layers.keys():             \n",
    "                x_dummy[k] = torch.squeeze(self.dummy_layers[k](v.int()),dim=1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        ##Process the base projections\n",
    "        x_proj = {}\n",
    "        for k,v in inputs.items(): \n",
    "            if k in x_cols_embed_group:\n",
    "                x_proj[k] = self.proj_layers[k](x_emdd[k])\n",
    "            elif k in x_cols_embed_no_group:\n",
    "                continue\n",
    "            elif k in txt_group:\n",
    "                continue\n",
    "            #elif k in txt_group_appr:\n",
    "                #continue\n",
    "            elif k not in ['sale_age_toanchr','sale_age_lag1','purprice_lag1','H8','geohash']:\n",
    "                x_proj[k] = self.proj_layers[k](v)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        x_temporal_out = {}\n",
    "        for k,v in self.temporal_grps.items():\n",
    "            x_proj['CLS_'+k] = self.proj_layers['CLS_'+k](inputs['CLS'])\n",
    "            x_temporal_in = torch.stack([x_proj[i] for i in self.temporal_grps[k]]+[x_proj['CLS_'+k]] ,dim=1)\n",
    "            x_positional =  getPositionEncoding(seq_len=len(v)+1, d=self.projSize, n=10000)\n",
    "            x_temporal_in += torch.tensor(x_positional)\n",
    "            temporal_attn,_ = self.temporal_layer_attn[k](x_temporal_in,x_temporal_in,x_temporal_in)\n",
    "            x_temporal_out[k] = self.temporal_layer_norm[k](temporal_attn)[:,0,:]\n",
    "        \n",
    "        x_spatial_time = []\n",
    "        for k in spatial_group:\n",
    "            x_spatial_time.append(self.spatial_time_layer[k](x_proj[k],xt)) #\n",
    "        \n",
    "        ##Prior Sale Projection\n",
    "        x_purprice_lag = self.proj_prior_sale(inputs['purprice_lag1'],xt_lag1)\n",
    "        \n",
    "        ##Dummies Projection\n",
    "        x_dummy = torch.cat(list(x_dummy.values()),-1)\n",
    "        x_dummy = self.proj_dummy(x_dummy)\n",
    "        \n",
    "        #process spatial attn\n",
    "        x_proj['CLS_spatial'] = self.proj_layers['CLS_spatial'](inputs['CLS'])\n",
    "        x_spatial = torch.stack([x_proj['CLS_spatial']]+x_spatial_time,dim=1)\n",
    "        x_spatial_saint  = self.saint_spatial(x_spatial)\n",
    "        \n",
    "        txt_vsn_out=self.vsn_layer_txt(x_txt[:2])\n",
    "        x_txt=[txt_vsn_out]+x_txt[2:]\n",
    "        x_proj['CLS_txt'] = self.proj_layers['CLS_txt'](inputs['CLS'])\n",
    "        x_txt = torch.stack([x_proj['CLS_txt']]+x_txt,dim=1)\n",
    "        \n",
    "        #x_proj['CLS_txt_appr'] = self.proj_layers['CLS_txt_appr'](inputs['CLS'])\n",
    "        #x_txt_appr = torch.stack([x_proj['CLS_txt_appr']]+x_txt_appr,dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##Process the VSN Blocks\n",
    "        x_vsn_out = {}\n",
    "        for k,v in self.vsn_grps.items():\n",
    "            if k =='other':\n",
    "                continue\n",
    "            else:    \n",
    "                x_vsn_in = [x_proj[c] for c in v]\n",
    "                x_vsn_out[k] = self.vsn_layer[k](x_vsn_in, context=x_spatial_saint)\n",
    "        #x_list  = x_vsn_out['list_price']\n",
    "        del x_vsn_out['list_price']\n",
    "        \n",
    "        if self.print_switch:\n",
    "            print(x_vsn_out.keys())\n",
    "            print(x_temporal_out.keys())\n",
    "            self.print_switch = False\n",
    "\n",
    "        #process feat attn\n",
    "        x_feat  = torch.stack([x_proj['CLS'],x_dummy]+list(x_vsn_out.values())+list(x_temporal_out.values()),dim=1) #+list(x_temporal_out.values()),x_dummy\n",
    "        #x_txt_appr_saint=self.saint_txt_appr(x_txt_appr)\n",
    "        x_feat_saint= self.saint_feat_inter(x_feat,x_txt,x_txt,context_k=x_spatial_saint,context_q=x_spatial_saint)+self.saint_feat(x_feat,context_k=x_spatial_saint,context_q=x_spatial_saint)\n",
    "        x_txt_saint  = self.saint_txt_inter(x_txt,x_feat,x_feat,context_k=x_spatial_saint,context_q=x_spatial_saint)+self.saint_txt(x_txt,context_k=x_spatial_saint,context_q=x_spatial_saint)\n",
    "        x_purprice_lag_saint=self.saint_purprice_lag(torch.stack([x_purprice_lag]+list(x_temporal_out.values()),dim=1),row_kv=x_feat_saint)\n",
    "        x_out = torch.stack([x_feat_saint,x_purprice_lag_saint,x_spatial_saint,x_txt_saint],dim=1)  #x_list #x_vsn_out['tax_values']\n",
    "        x_out = torch.flatten(self.relu(self.avg_p_cv(self.conv(x_out))),start_dim=1)#.unsqueeze(1),.squeeze(1)\n",
    "        x_out = self.proj_feat(x_out)\n",
    "        ##Concat temporal and remaining feats\n",
    "        x_out = torch.concat([xt,x_out],axis=1) #,xq\n",
    "        \n",
    "        ##output projection\n",
    "        if epoch >= self.start_smooth and fsd:\n",
    "                x_out = self.FDS.smooth(x_out, targets, epoch)#.permute(1, 0, 2)\n",
    "        \n",
    "        mew = self.mlp_feat(x_out)\n",
    "        #sigma = torch.log(1 +torch.exp(self.out_mlp_sigma(x_out)))\n",
    "        return mew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_pc10_adaptive(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(loss_pc10_adaptive, self).__init__()\n",
    "        self.softmax=nn.Softmax()\n",
    "    def forward(self, predictions, target):\n",
    "        loss_value=0\n",
    "        pred=torch.exp(predictions)\n",
    "        label=torch.exp(target)\n",
    "        #weight=self.softmax(torch.neg(target))\n",
    "        length=list(predictions.size())[0]\n",
    "        for i in range(length):\n",
    "            ape=torch.abs(1-predictions[i]/target[i])\n",
    "            if torch.abs(1-pred[i]/label[i])>0.1:\n",
    "                    base_loss=ape\n",
    "            else:\n",
    "                base_loss=(40*(torch.square(torch.abs(predictions[i]-target[i])))+0.6)*ape\n",
    "            loss_value+=base_loss\n",
    "        return loss_value/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quantile_loss(nn.Module):\n",
    "    def __init__(self, quantiles):\n",
    "        super(quantile_loss, self).__init__()\n",
    "        self.quantiles = quantiles\n",
    "        \n",
    "    # Loss function\n",
    "    def get_one_quantile(self, y_pred, y_target, quantile):\n",
    "        if quantile < 0 or quantile > 1:\n",
    "            raise ValueError(\n",
    "                'Illegal quantile value={}! Values should be between 0 and 1.'.format(quantile))\n",
    "\n",
    "        pred_diff = y_target - y_pred\n",
    "        loss = quantile * torch.max(pred_diff, torch.zeros_like(pred_diff)) + (1. - quantile) * torch.max(-pred_diff, torch.zeros_like(pred_diff))\n",
    "        return loss.unsqueeze(-1)\n",
    "    def forward(self, preds, targets):\n",
    "        loss = []\n",
    "        for i, quantile in enumerate(self.quantiles):\n",
    "            loss.append(self.get_one_quantile(preds[..., i],targets, quantile))\n",
    "        q_loss = torch.mean(torch.sum(torch.cat(loss, axis = -1), axis = -1))\n",
    "        return q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-auth==2.22.0\n",
    "# ! pip install tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%%time\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Configuration dictionary and model initialization\n",
    "config = {\n",
    "    'dummy_dp': 0.15,\n",
    "    'vsn_dp': 0.05,\n",
    "    'spatial_time_dp': 0.1,\n",
    "    'prior_sale_dp': 0.03,\n",
    "    'proj_dp': 0.1\n",
    "}\n",
    "\n",
    "model = nn_vsn_saint(config, 1, 45, input_dict, var_grps, temporal_grps)\n",
    "model = model.float()\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_ap10 = loss_pc10_adaptive()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', threshold=1e-3, factor=.5, patience=0, min_lr=1e-7)\n",
    "\n",
    "def rmse_loss(logits, labels):\n",
    "    return torch.sqrt(F.mse_loss(logits, labels.reshape(-1, 1)))\n",
    "\n",
    "def on_before_optimizer_step(optimizer, writer, global_step):\n",
    "    if global_step % 25 == 0:  # Log every 25 steps\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"grad/{name}\", param.grad, global_step)\n",
    "                grad_norm = torch.linalg.vector_norm(param.grad).item()\n",
    "                writer.add_scalar(f\"grad/{name}_norm\", grad_norm, global_step)\n",
    "\n",
    "writer = SummaryWriter('runs/experiment_v6')\n",
    "\n",
    "num_epochs = 50\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_pc10 = 0.0\n",
    "    running_rmse = 0.0\n",
    "    running_batch_cnt = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader_MLP):\n",
    "        inputs_t, target_t = data\n",
    "        optimizer.zero_grad()\n",
    "        preds_t = model(inputs_t, target_t, epoch, fsd=True)\n",
    "\n",
    "        # Loss calculations\n",
    "        loss = loss_ap10(preds_t, target_t.float())\n",
    "        rmse = rmse_loss(preds_t, target_t.float())\n",
    "        pc10 = pc10_accm_torch(torch.exp(target_t), torch.exp(preds_t))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_rmse += rmse.item()  # Accumulate RMSE loss\n",
    "        running_pc10 += pc10.item()\n",
    "        running_batch_cnt += target_t.shape[0]\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        on_before_optimizer_step(optimizer, writer, global_step)  # Log gradients\n",
    "        optimizer.step()\n",
    "        global_step += 1  # Increment global step\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader_MLP)\n",
    "    avg_rmse = running_rmse / len(train_loader_MLP)  # Average RMSE loss\n",
    "    avg_pc10 = running_pc10 / running_batch_cnt\n",
    "\n",
    "    # Log training metrics\n",
    "    writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
    "    writer.add_scalar('RMSE/Train', avg_rmse, epoch)  # Log average RMSE loss\n",
    "    writer.add_scalar('PC10/Train', avg_pc10, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss_v = 0\n",
    "    running_rmse_v = 0.0  # To accumulate RMSE loss for validation\n",
    "    running_pc10_v = 0.0\n",
    "    running_batch_cnt_v = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader_MLP):\n",
    "            inputs_v, target_v = data\n",
    "            preds_v = model(inputs_v, target_v, epoch)\n",
    "            loss_v = loss_ap10(preds_v, target_v.float())\n",
    "            rmse_v = rmse_loss(preds_v, target_v.float())\n",
    "            vpc10 = pc10_accm_torch(torch.exp(target_v), torch.exp(preds_v))\n",
    "\n",
    "            running_loss_v += loss_v.item()\n",
    "            running_rmse_v += rmse_v.item()  # Accumulate RMSE loss\n",
    "            running_pc10_v += vpc10.item()\n",
    "            running_batch_cnt_v += target_v.shape[0]\n",
    "\n",
    "        avg_vloss = running_loss_v / len(val_loader_MLP)\n",
    "        avg_vrmse = running_rmse_v / len(val_loader_MLP)  # Average RMSE loss for validation\n",
    "        avg_vpc10 = running_pc10_v / running_batch_cnt_v\n",
    "\n",
    "        # Log validation metrics\n",
    "        writer.add_scalar('Loss/Validation', avg_vloss, epoch)\n",
    "        writer.add_scalar('RMSE/Validation', avg_vrmse, epoch)  # Log average RMSE loss for validation\n",
    "        writer.add_scalar('PC10/Validation', avg_vpc10, epoch)\n",
    "\n",
    "    scheduler.step(avg_vpc10)\n",
    "    print(f'Epoch {epoch}, Loss Train {avg_loss}, RMSE Train {avg_rmse}, PC10 Train {avg_pc10}, Loss Val {avg_vloss}, RMSE Val {avg_vrmse}, PC10 Val {avg_vpc10}')\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "# %%time\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Configuration dictionary and model initialization\n",
    "# config = {\n",
    "#     'dummy_dp': 0.15,\n",
    "#     'vsn_dp': 0.05,\n",
    "#     'spatial_time_dp': 0.1,\n",
    "#     'prior_sale_dp': 0.03,\n",
    "#     'proj_dp': 0.1\n",
    "# }\n",
    "\n",
    "# model = nn_vsn_saint(config, 1, 45, input_dict, var_grps, temporal_grps)\n",
    "# model = model.float()\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# loss_ap10 = loss_pc10_adaptive()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', threshold=1e-3, factor=.5, patience=0, min_lr=1e-7)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter('runs/experiment_v2'1\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0\n",
    "#     running_pc10 = 0.0\n",
    "#     running_batch_cnt = 0.0\n",
    "\n",
    "#     for i, data in enumerate(train_loader_MLP):\n",
    "#         inputs_t, target_t = data\n",
    "#         optimizer.zero_grad()\n",
    "#         preds_t = model(inputs_t, target_t, epoch, fsd=True)\n",
    "#         loss = loss_ap10(preds_t, target_t.float())\n",
    "#         pc10 = pc10_accm_torch(torch.exp(target_t), torch.exp(preds_t))\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         running_pc10 += pc10.item()\n",
    "#         running_batch_cnt += target_t.shape[0]\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     avg_loss = running_loss / (i + 1)\n",
    "#     avg_pc10 = running_pc10 / running_batch_cnt\n",
    "\n",
    "#     # Log training metrics\n",
    "#     writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
    "#     writer.add_scalar('PC10/Train', avg_pc10, epoch)\n",
    "\n",
    "#     model.eval()\n",
    "#     running_loss_v = 0\n",
    "#     running_pc10_v = 0.0\n",
    "#     running_batch_cnt_v = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(val_loader_MLP):\n",
    "#             inputs_v, target_v = data\n",
    "#             preds_v = model(inputs_v, target_v, epoch)\n",
    "#             loss_v = loss_ap10(preds_v, target_v.float())\n",
    "#             vpc10 = pc10_accm_torch(torch.exp(target_v), torch.exp(preds_v))\n",
    "\n",
    "#             running_loss_v += loss_v.item()\n",
    "#             running_pc10_v += vpc10.item()\n",
    "#             running_batch_cnt_v += target_v.shape[0]\n",
    "\n",
    "#         avg_vloss = running_loss_v / (i + 1)\n",
    "#         avg_vpc10 = running_pc10_v / running_batch_cnt_v\n",
    "\n",
    "#         # Log validation metrics\n",
    "#         writer.add_scalar('Loss/Validation', avg_vloss, epoch)\n",
    "#         writer.add_scalar('PC10/Validation', avg_vpc10, epoch)\n",
    "\n",
    "#     scheduler.step(avg_vpc10)\n",
    "#     print(f'Epoch {epoch}, Loss Train {avg_loss}, Loss Val {avg_vloss}, PC10 Train {avg_pc10}, PC10 Val {avg_vpc10}')\n",
    "\n",
    "# # Close the writer\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss Train 3.4489622547461822, RMSE Train 1.1418155016483846, PC10 Train 0.08644138398972787, Loss Val 1.446762735779221, RMSE Val 1.0297294549039893, PC10 Val 0.09770425910180104\n",
      "Epoch 1, Loss Train 2.369164626519601, RMSE Train 1.0617725471476536, PC10 Train 0.09143573689250761, Loss Val 1.0813624633325112, RMSE Val 0.9933193970370937, PC10 Val 0.10218752415552292\n",
      "Epoch 2, Loss Train 1.8049958197920173, RMSE Train 1.0334603534088478, PC10 Train 0.09403812542138511, Loss Val 1.3358792614292454, RMSE Val 1.0092178357614052, PC10 Val 0.10110535672876246\n",
      "Epoch 3, Loss Train 1.6540904378031824, RMSE Train 1.025036679373847, PC10 Train 0.09610371763655025, Loss Val 1.3124680937947453, RMSE Val 0.9969701718639683, PC10 Val 0.10137589858545258\n",
      "Epoch 4, Loss Train 1.5545042907153521, RMSE Train 1.00428631320014, PC10 Train 0.09675216758351476, Loss Val 0.98933144517847, RMSE Val 0.9961666577571148, PC10 Val 0.0992888614052717\n",
      "Epoch 5, Loss Train 1.0418910099579408, RMSE Train 1.000553159205405, PC10 Train 0.09796747443775954, Loss Val 1.055300838238484, RMSE Val 0.9971911971633499, PC10 Val 0.100023189302002\n",
      "Epoch 6, Loss Train 1.0430621334740349, RMSE Train 1.000187542166438, PC10 Train 0.0981048942940699, Loss Val 0.9866576710262814, RMSE Val 0.9956825826619122, PC10 Val 0.10025508232202211\n",
      "Epoch 7, Loss Train 1.0231854468136579, RMSE Train 0.9992051863813544, PC10 Train 0.098474210157904, Loss Val 0.9796001218460701, RMSE Val 0.9961258849582156, PC10 Val 0.10029373115869213\n",
      "Epoch 8, Loss Train 0.9926608081336494, RMSE Train 0.9994034427064317, PC10 Train 0.09803618436591473, Loss Val 0.9812131443539182, RMSE Val 0.9961373467703123, PC10 Val 0.0995207544252918\n",
      "Epoch 9, Loss Train 0.9842758561755802, RMSE Train 0.9995026072940311, PC10 Train 0.098117777405599, Loss Val 0.9814835239101101, RMSE Val 0.9961586272394335, PC10 Val 0.10029373115869213\n",
      "Epoch 10, Loss Train 0.9873272694266952, RMSE Train 0.9994906382159786, PC10 Train 0.09791164762113345, Loss Val 0.9799997033299627, RMSE Val 0.9962219998643205, PC10 Val 0.09986859395532194\n",
      "Epoch 11, Loss Train 0.9834438846991943, RMSE Train 0.9992756254680164, PC10 Train 0.0977527559122746, Loss Val 0.9806578884253631, RMSE Val 0.9962335905513248, PC10 Val 0.0992888614052717\n",
      "Epoch 12, Loss Train 0.982283661494384, RMSE Train 0.999537785311003, PC10 Train 0.09750797679322176, Loss Val 0.9803349859005696, RMSE Val 0.9962835183014741, PC10 Val 0.09955940326196182\n",
      "Epoch 13, Loss Train 0.9831832518448701, RMSE Train 0.9993870682544537, PC10 Train 0.09773557843023581, Loss Val 0.9802335178529894, RMSE Val 0.9958099610096699, PC10 Val 0.09963670093530184\n",
      "Epoch 14, Loss Train 0.9813500832508992, RMSE Train 0.9995472173433047, PC10 Train 0.0977570502827843, Loss Val 0.9802456691458419, RMSE Val 0.9961297205976538, PC10 Val 0.09963670093530184\n",
      "Epoch 15, Loss Train 0.9826426686825337, RMSE Train 0.999319986717121, PC10 Train 0.09778281650584249, Loss Val 0.980289465672261, RMSE Val 0.9960866554363353, PC10 Val 0.09959805209863183\n",
      "Epoch 16, Loss Train 0.9814922360328583, RMSE Train 0.9993990876295187, PC10 Train 0.09767545724310002, Loss Val 0.980282765788001, RMSE Val 0.996231771804191, PC10 Val 0.09959805209863183\n",
      "Epoch 17, Loss Train 0.9821116809730415, RMSE Train 0.9993573769076809, PC10 Train 0.09765827976106123, Loss Val 0.980279761391717, RMSE Val 0.9958944916725159, PC10 Val 0.09959805209863183\n",
      "Epoch 18, Loss Train 0.9825482205585675, RMSE Train 0.9994657512183662, PC10 Train 0.09767545724310002, Loss Val 0.9802784774754498, RMSE Val 0.996006512964094, PC10 Val 0.09959805209863183\n",
      "Epoch 19, Loss Train 0.9818366035684809, RMSE Train 0.9993351746249843, PC10 Train 0.09771410657768731, Loss Val 0.9802650261569668, RMSE Val 0.9961178302764893, PC10 Val 0.09955940326196182\n",
      "Epoch 20, Loss Train 0.9829125211045548, RMSE Train 0.9993701186982004, PC10 Train 0.09767116287259033, Loss Val 0.9802683076343021, RMSE Val 0.9962648121086327, PC10 Val 0.09955940326196182\n",
      "Epoch 21, Loss Train 0.9818564467243962, RMSE Train 0.9995049907638504, PC10 Train 0.0977441671712552, Loss Val 0.9802782761084067, RMSE Val 0.9961111062281841, PC10 Val 0.09955940326196182\n",
      "Epoch 22, Loss Train 0.9815514905674679, RMSE Train 0.9993668017802654, PC10 Train 0.09770122346615821, Loss Val 0.9802830992518244, RMSE Val 0.9961492999179943, PC10 Val 0.09959805209863183\n",
      "Epoch 23, Loss Train 0.9811478162313009, RMSE Train 0.9992952278784445, PC10 Train 0.09767116287259033, Loss Val 0.9802920480032224, RMSE Val 0.9955627177212689, PC10 Val 0.09959805209863183\n",
      "Epoch 24, Loss Train 0.9817337391970752, RMSE Train 0.9994649468241511, PC10 Train 0.0977269896892164, Loss Val 0.9802880174404865, RMSE Val 0.9959815853350872, PC10 Val 0.09963670093530184\n",
      "Epoch 25, Loss Train 0.9819106427398888, RMSE Train 0.9995084430004383, PC10 Train 0.09758956983290604, Loss Val 0.9802845829241985, RMSE Val 0.9960960552499101, PC10 Val 0.09959805209863183\n",
      "Epoch 26, Loss Train 0.9824161653046135, RMSE Train 0.999404810033403, PC10 Train 0.09765398539055152, Loss Val 0.9802893690160803, RMSE Val 0.9963470375215685, PC10 Val 0.09959805209863183\n",
      "Epoch 27, Loss Train 0.9823344283633761, RMSE Train 0.9994876855486506, PC10 Train 0.09768834035462912, Loss Val 0.9803084537789628, RMSE Val 0.995903996196953, PC10 Val 0.09959805209863183\n",
      "Epoch 28, Loss Train 0.9825975864499181, RMSE Train 0.9992971624340024, PC10 Train 0.09767116287259033, Loss Val 0.9802850807035292, RMSE Val 0.995736222009401, PC10 Val 0.09959805209863183\n",
      "Epoch 29, Loss Train 0.9817734360694885, RMSE Train 0.9994748352884172, PC10 Train 0.0977441671712552, Loss Val 0.98029769111324, RMSE Val 0.9960805548203958, PC10 Val 0.09959805209863183\n",
      "Epoch 30, Loss Train 0.9819647798667083, RMSE Train 0.9993128551019205, PC10 Train 0.09763680790851273, Loss Val 0.9803077063044986, RMSE Val 0.9961105150145453, PC10 Val 0.09959805209863183\n",
      "Epoch 31, Loss Train 0.9817564002386443, RMSE Train 0.999444571701256, PC10 Train 0.09758098109188665, Loss Val 0.9802837210732538, RMSE Val 0.996269358170999, PC10 Val 0.09959805209863183\n",
      "Epoch 32, Loss Train 0.9810896122061813, RMSE Train 0.9993067233412115, PC10 Train 0.09770551783666791, Loss Val 0.9803017426181484, RMSE Val 0.9963319543245677, PC10 Val 0.09955940326196182\n",
      "Epoch 33, Loss Train 0.9816931507608913, RMSE Train 0.9995011325713035, PC10 Train 0.09755092049831876, Loss Val 0.9803078335684698, RMSE Val 0.9960712532739382, PC10 Val 0.09955940326196182\n",
      "Epoch 34, Loss Train 0.9829806549413068, RMSE Train 0.9994583410901708, PC10 Train 0.09760674731494484, Loss Val 0.9803026834049741, RMSE Val 0.9960203750713451, PC10 Val 0.09955940326196182\n",
      "Epoch 35, Loss Train 0.9813445874520609, RMSE Train 0.9993835922118064, PC10 Train 0.0977742277648231, Loss Val 0.9802907109260559, RMSE Val 0.9956740833617546, PC10 Val 0.09959805209863183\n",
      "Epoch 36, Loss Train 0.9813745966902724, RMSE Train 0.9994528159364924, PC10 Train 0.09771410657768731, Loss Val 0.9802860714293815, RMSE Val 0.9962764044065733, PC10 Val 0.09959805209863183\n",
      "Epoch 37, Loss Train 0.9823228657782614, RMSE Train 0.9994132635113713, PC10 Train 0.09772269531870671, Loss Val 0.9802823098930152, RMSE Val 0.9960802696846627, PC10 Val 0.09959805209863183\n",
      "Epoch 38, Loss Train 0.9819142796971776, RMSE Train 0.9994082733675524, PC10 Train 0.097718400948197, Loss Val 0.9802939086347013, RMSE Val 0.9962582314336622, PC10 Val 0.09959805209863183\n",
      "Epoch 39, Loss Train 0.9834697695823761, RMSE Train 0.9994731523969151, PC10 Train 0.09762821916749333, Loss Val 0.9802939907924549, RMSE Val 0.9960313213838113, PC10 Val 0.09959805209863183\n",
      "Epoch 40, Loss Train 0.9818600353535947, RMSE Train 0.9993647827400459, PC10 Train 0.09764110227902242, Loss Val 0.9802961687783938, RMSE Val 0.9958414274293024, PC10 Val 0.09959805209863183\n",
      "Epoch 41, Loss Train 0.982005893110155, RMSE Train 0.9993217338670839, PC10 Train 0.09764539664953213, Loss Val 0.9803011691248095, RMSE Val 0.9959769055650041, PC10 Val 0.09959805209863183\n",
      "Epoch 42, Loss Train 0.9813713676578648, RMSE Train 0.999407905179101, PC10 Train 0.0977484615417649, Loss Val 0.9802754972432111, RMSE Val 0.9961608181128631, PC10 Val 0.09967534977197186\n",
      "Epoch 43, Loss Train 0.9810275635919772, RMSE Train 0.9993543159496319, PC10 Train 0.09767545724310002, Loss Val 0.9802600983026866, RMSE Val 0.9959082442360956, PC10 Val 0.09971399860864189\n",
      "Epoch 44, Loss Train 0.982509508326247, RMSE Train 0.9994400793367678, PC10 Train 0.09783434895195887, Loss Val 0.9802796679574091, RMSE Val 0.996284955256694, PC10 Val 0.09963670093530184\n",
      "Epoch 45, Loss Train 0.9824095148224015, RMSE Train 0.999392197475777, PC10 Train 0.09759386420341574, Loss Val 0.9802759644147512, RMSE Val 0.9959339499473572, PC10 Val 0.09967534977197186\n",
      "Epoch 46, Loss Train 0.9819741401228461, RMSE Train 0.9993869681974074, PC10 Train 0.09764110227902242, Loss Val 0.9802914132943025, RMSE Val 0.9962324467865197, PC10 Val 0.09971399860864189\n",
      "Epoch 47, Loss Train 0.9814002537154578, RMSE Train 0.9993952235302052, PC10 Train 0.09760674731494484, Loss Val 0.9802643157340385, RMSE Val 0.996169579995645, PC10 Val 0.09971399860864189\n",
      "Epoch 48, Loss Train 0.9819978303737469, RMSE Train 0.9993493843365002, PC10 Train 0.09761963042647394, Loss Val 0.9802777010041315, RMSE Val 0.9959825067906767, PC10 Val 0.09967534977197186\n",
      "Epoch 49, Loss Train 0.9821068551447298, RMSE Train 0.9994511224844076, PC10 Train 0.09764969102004183, Loss Val 0.9802707095403929, RMSE Val 0.9960305433015566, PC10 Val 0.09967534977197186\n",
      "CPU times: user 2d 10h 5min 34s, sys: 10h 48min 39s, total: 2d 20h 54min 14s\n",
      "Wall time: 6h 42min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "# %%time\n",
    "# #torch.autograd.set_detect_anomaly(True)\n",
    "# config={'dummy_dp': 0.15, 'vsn_dp': 0.05, 'spatial_time_dp': 0.1, 'prior_sale_dp': 0.03, 'proj_dp': 0.1}\n",
    "# ##Instantiate the model object\n",
    "# model = nn_vsn_saint(config,1,45,input_dict,var_grps,temporal_grps)\n",
    "# model= model.float()\n",
    "# #model_weights = torch.load(f\"/home/jovyan/geohash_index_output/Tab_transformer_customhve/CA/tabtransformer_{state[0]}_ALL_6.5_ple{ple_size}_{time}_17.pth\",map_location=torch.device('cpu'))\n",
    "# #model.load_state_dict(model_weights['model_state_dict'])\n",
    "# ##Define the optimization plan\n",
    "\n",
    "# #loss_mse = torch.nn.MSELoss()\n",
    "# loss_ap10 = loss_pc10_adaptive()\n",
    "\n",
    "# #loss_gnll = torch.nn.GaussianNLLLoss()\n",
    "# l1_lambda = 0.05\n",
    "# optimizer  = torch.optim.AdamW(model.parameters(), lr=0.005,weight_decay=0.01)#torch.optim.RAdam(model.parameters(), lr=0.005)#\n",
    "# ##Define  Learning Rate Schedueler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', threshold =1e-3,factor=.5, patience =0, min_lr= 1e-7)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for p in optimizer.param_groups:\n",
    "#         return p[\"lr\"]\n",
    "# ##Eexecute the training loop\n",
    "# ##Eexecute the training loop\n",
    "# num_epochs = 50\n",
    "\n",
    "# ##Early stopping counter\n",
    "# estopCntr = 0\n",
    "# estopCntrLmt = 7\n",
    "# estopBestVal = 0\n",
    "# scoreBest = 0 \n",
    "# current_vpc10=0\n",
    "# for epoch in range(num_epochs):\n",
    "#     #start_time = time.time()\n",
    "#     model.train()\n",
    "#     running_loss = 0 \n",
    "#     running_pc10 = 0.0\n",
    "#     running_batch_cnt=0.0\n",
    "#     for i, data in enumerate(train_loader_MLP):\n",
    "#     #for i, data in enumerate(train_loader_MLP):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs_t,target_t = data\n",
    "#         preds_t= model(inputs_t,target_t,epoch,fsd=True)\n",
    "#         loss=loss_ap10(preds_t*trgt_s+trgt_mean,target_t.float()*trgt_s+trgt_mean)     \n",
    "#         pc10=pc10_accm_torch(torch.exp(target_t*trgt_s+trgt_mean),torch.exp(preds_t*trgt_s+trgt_mean))\n",
    "#         running_loss += loss.item()\n",
    "#         running_pc10 += pc10.item()\n",
    "#         running_batch_cnt +=  target_t.shape[0]             \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         gc.collect()\n",
    "#     avg_loss = running_loss / (i + 1)\n",
    "#     avg_pc10 = running_pc10 / running_batch_cnt\n",
    "#     ##Model Evaluations on Validation and Test for current Epocj\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         ##Performance Stats on Validation Data\n",
    "#         running_loss_v = 0 \n",
    "#         running_pc10_v = 0.0\n",
    "#         running_pc10plus_v = 0.0\n",
    "#         running_batch_cnt_v = 0.0\n",
    "#         for i, data in enumerate(val_loader_MLP):\n",
    "#             inputs_v,target_v = data\n",
    "#             preds_v = model(inputs_v,target_v,epoch)\n",
    "#             loss_v =loss_ap10(preds_v*trgt_s+trgt_mean,target_v.float()*trgt_s+trgt_mean)\n",
    "#             vpc10=pc10_accm_torch(torch.exp(target_v*trgt_s+trgt_mean),torch.exp(preds_v*trgt_s+trgt_mean))\n",
    "#             vpc10plus = pc10plus_accm_torch(torch.exp(target_v*trgt_s+trgt_mean),torch.exp(preds_v*trgt_s+trgt_mean))\n",
    "#             running_loss_v += loss_v.item()\n",
    "#             running_pc10_v += vpc10.item()\n",
    "#             running_pc10plus_v += vpc10plus.item()\n",
    "#             running_batch_cnt_v +=  target_v.shape[0]\n",
    "#         avg_vloss = running_loss_v / (i + 1)\n",
    "#         avg_vpc10 = running_pc10_v / running_batch_cnt_v\n",
    "#         avg_vpc10plus = running_pc10plus_v / running_batch_cnt_v\n",
    "#         ##Performance Stats on Testing Data\n",
    "#         running_loss_test = 0 \n",
    "#         running_pc10_test = 0.0\n",
    "#         running_pc10plus_test = 0.0\n",
    "#         running_batch_cnt_test = 0.0\n",
    "#         for i, data in enumerate(test_loader_MLP):\n",
    "#             inputs_test,target_test = data\n",
    "#             preds_test = model(inputs_test,target_test,epoch)\n",
    "#             loss_test =loss_ap10(preds_test*trgt_s+trgt_mean,target_test.float()*trgt_s+trgt_mean)\n",
    "#             testpc10=pc10_accm_torch(torch.exp(target_test*trgt_s+trgt_mean),torch.exp(preds_test*trgt_s+trgt_mean))\n",
    "#             testpc10plus = pc10plus_accm_torch(torch.exp(target_test*trgt_s+trgt_mean),torch.exp(preds_test*trgt_s+trgt_mean))\n",
    "#             running_loss_test += loss_test.item()\n",
    "#             running_pc10_test += testpc10.item()\n",
    "#             running_pc10plus_test += testpc10plus.item()\n",
    "#             running_batch_cnt_test +=  target_test.shape[0]\n",
    "#         avg_testloss = running_loss_test / (i + 1)\n",
    "#         avg_testpc10 = running_pc10_test/ running_batch_cnt_test\n",
    "#         avg_testpc10plus = running_pc10plus_test / running_batch_cnt_test\n",
    "#     #old_vpc10=current_vpc10\n",
    "#     # current_vpc10=avg_vpc10\n",
    "#     ##Apply scheduelers and early stopping\n",
    "#     scheduler.step(avg_vpc10)\n",
    "#     print('Epoch {} , LOSS Train {} ,LOSS val {}, LOSS test {}, pc10_train {} ,pc10_val {}, pc10_test {},  pc10plus_test {} ,LR Next Epoch {} '.format(epoch,avg_loss,avg_vloss, avg_testloss, avg_pc10 ,avg_vpc10 , avg_testpc10, avg_testpc10plus, get_lr(scheduler.optimizer)))   \n",
    "#     if avg_vpc10 > estopBestVal:\n",
    "#         best_mdl = copy.deepcopy(model)\n",
    "#         best_epoch = epoch\n",
    "#         estopBestVal = avg_vpc10\n",
    "#         estopCntr = 0\n",
    "#     else:\n",
    "#         estopCntr += 1\n",
    "#         print('Estop Cntr Increased to ' , estopCntr)\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     if (get_lr(scheduler.optimizer)==1e-7) or (estopCntr >= estopCntrLmt) :\n",
    "#         print('*****Early Stopping Limit Reached******')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , LOSS Train 0.017048900927397404 ,LOSS val 0.010223837544185084, LOSS test 0.013514482774413548, pc10_train 0.4365957666095515 ,pc10_val 0.5831336476772049, pc10_test 0.5239772727272727,  pc10plus_test 0.27375 ,LR Next Epoch 0.005 \n",
      "Epoch 1 , LOSS Train 0.01022674687564731 ,LOSS val 0.010750500535642778, LOSS test 0.013700132114956012, pc10_train 0.577442530586654 ,pc10_val 0.5421658808069877, pc10_test 0.4709090909090909,  pc10plus_test 0.12056818181818182 ,LR Next Epoch 0.0025 \n",
      "Estop Cntr Increased to  1\n",
      "Epoch 2 , LOSS Train 0.009306732505034129 ,LOSS val 0.009338595375821396, LOSS test 0.011356300244537683, pc10_train 0.6210303912600972 ,pc10_val 0.6274638633377135, pc10_test 0.5939772727272727,  pc10plus_test 0.2109090909090909 ,LR Next Epoch 0.0025 \n",
      "Epoch 3 , LOSS Train 0.008885973974972873 ,LOSS val 0.009471622346019422, LOSS test 0.011481586198967237, pc10_train 0.6369668002215895 ,pc10_val 0.6229033006106516, pc10_test 0.5877272727272728,  pc10plus_test 0.18738636363636363 ,LR Next Epoch 0.00125 \n",
      "Estop Cntr Increased to  1\n",
      "Epoch 4 , LOSS Train 0.008249838001656908 ,LOSS val 0.009256882492352176, LOSS test 0.011092581356374117, pc10_train 0.6666451948141182 ,pc10_val 0.6379763469119579, pc10_test 0.6077272727272728,  pc10plus_test 0.17852272727272728 ,LR Next Epoch 0.00125 \n",
      "Epoch 5 , LOSS Train 0.007918008935970586 ,LOSS val 0.00942640847249611, LOSS test 0.011661075986921787, pc10_train 0.6806491370462461 ,pc10_val 0.6235603308340419, pc10_test 0.5839772727272727,  pc10plus_test 0.24829545454545454 ,LR Next Epoch 0.000625 \n",
      "Estop Cntr Increased to  1\n",
      "Epoch 6 , LOSS Train 0.007464495621755853 ,LOSS val 0.009209760741607563, LOSS test 0.01120913472886269, pc10_train 0.7023271193792058 ,pc10_val 0.64296204684239, pc10_test 0.6055681818181818,  pc10plus_test 0.19670454545454547 ,LR Next Epoch 0.000625 \n",
      "Epoch 7 , LOSS Train 0.007230346821047165 ,LOSS val 0.009188064902618125, LOSS test 0.011154738039924549, pc10_train 0.7138875647913151 ,pc10_val 0.6439669165958105, pc10_test 0.6111363636363636,  pc10plus_test 0.2040909090909091 ,LR Next Epoch 0.000625 \n",
      "Epoch 8 , LOSS Train 0.007063669339941071 ,LOSS val 0.009314831472127824, LOSS test 0.011353811177496727, pc10_train 0.7208530337580465 ,pc10_val 0.6379376980752879, pc10_test 0.6039772727272728,  pc10plus_test 0.21386363636363637 ,LR Next Epoch 0.0003125 \n",
      "Estop Cntr Increased to  1\n",
      "Epoch 9 , LOSS Train 0.006767480919422867 ,LOSS val 0.009245925884995912, LOSS test 0.01130937047016162, pc10_train 0.7359133911355604 ,pc10_val 0.6441601607791605, pc10_test 0.6082954545454545,  pc10plus_test 0.21738636363636363 ,LR Next Epoch 0.00015625 \n",
      "Epoch 10 , LOSS Train 0.0065489296770623855 ,LOSS val 0.009266901547340927, LOSS test 0.011310801053276429, pc10_train 0.7472161743170878 ,pc10_val 0.6423436654556698, pc10_test 0.6023863636363637,  pc10plus_test 0.22920454545454547 ,LR Next Epoch 7.8125e-05 \n",
      "Estop Cntr Increased to  1\n",
      "Epoch 11 , LOSS Train 0.006443286648820023 ,LOSS val 0.009282865486032254, LOSS test 0.011310863451888928, pc10_train 0.7531681718435304 ,pc10_val 0.6417252840689496, pc10_test 0.6097727272727272,  pc10plus_test 0.21568181818181817 ,LR Next Epoch 3.90625e-05 \n",
      "Estop Cntr Increased to  2\n",
      "Epoch 12 , LOSS Train 0.006381890399510169 ,LOSS val 0.009272404644336249, LOSS test 0.011262019666341634, pc10_train 0.756380360984785 ,pc10_val 0.6435804282291103, pc10_test 0.6132954545454545,  pc10plus_test 0.21522727272727274 ,LR Next Epoch 1.953125e-05 \n",
      "Estop Cntr Increased to  3\n",
      "Epoch 13 , LOSS Train 0.0063584623019303285 ,LOSS val 0.009282230777112214, LOSS test 0.011276645514254387, pc10_train 0.7580723429656064 ,pc10_val 0.6431552910257401, pc10_test 0.6113636363636363,  pc10plus_test 0.215 ,LR Next Epoch 9.765625e-06 \n",
      "Estop Cntr Increased to  4\n",
      "Epoch 14 , LOSS Train 0.006338204567631085 ,LOSS val 0.009276489450319393, LOSS test 0.01129476038309244, pc10_train 0.7584287757179113 ,pc10_val 0.6424982608023498, pc10_test 0.6107954545454546,  pc10plus_test 0.21636363636363637 ,LR Next Epoch 4.8828125e-06 \n",
      "Estop Cntr Increased to  5\n",
      "Epoch 15 , LOSS Train 0.006335036980139243 ,LOSS val 0.009279527273532507, LOSS test 0.01127733182735168, pc10_train 0.7586950266895127 ,pc10_val 0.6423823142923398, pc10_test 0.6117045454545454,  pc10plus_test 0.21693181818181817 ,LR Next Epoch 2.44140625e-06 \n",
      "Estop Cntr Increased to  6\n",
      "Epoch 16 , LOSS Train 0.0063300742362220365 ,LOSS val 0.009282100845027614, LOSS test 0.011295955843077255, pc10_train 0.7591588187045601 ,pc10_val 0.6424596119656798, pc10_test 0.6097727272727272,  pc10plus_test 0.2197727272727273 ,LR Next Epoch 1.220703125e-06 \n",
      "Estop Cntr Increased to  7\n",
      "*****Early Stopping Limit Reached******\n",
      "CPU times: user 2d 20h 5s, sys: 3h 23min 51s, total: 2d 23h 23min 57s\n",
      "Wall time: 3h 40min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n"
     ]
    }
   ],
   "source": [
    "# output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model ##\n",
    "# path = f\"/home/jovyan/geohash_index_output/Tab_transformer_customhve/CA/tabtransformer_{state[0]}_ALL_7.6_ple{ple_size}_{time}_{best_epoch}.pth\"\n",
    "path = f\"/home/jovyan/geohash_index_output/tab_transformer_customhve/tabtransformer_{state[0]}_ALL_7.6_ple{ple_size}_{time}_{best_epoch}.pth\"\n",
    "\n",
    "try:\n",
    "    state_dict = best_mdl.module.state_dict()\n",
    "    \n",
    "except AttributeError:\n",
    "    state_dict = best_mdl.state_dict()\n",
    "torch.save({'model_state_dict': state_dict}, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:15<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "##Get Performance Summary on Test Set##\n",
    "config={'dummy_dp': 0.15, 'vsn_dp': 0.03, 'spatial_time_dp': 0.1, 'prior_sale_dp': 0.03, 'proj_dp': 0.1}\n",
    "best_mdl = nn_vsn_saint(config,1,45,input_dict,var_grps,temporal_grps)\n",
    "best_mdl= best_mdl.float()\n",
    "model_weights = torch.load(f\"/home/jovyan/geohash_index_output/tab_transformer_customhve/tabtransformer_{state[0]}_ALL_7.6_ple{ple_size}_{time}_{best_epoch}.pth\",map_location=torch.device('cpu'))\n",
    "best_mdl.load_state_dict(model_weights['model_state_dict'])\n",
    "best_mdl.eval() #Best Model object\n",
    "predicts_test=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(tqdm(test_loader_MLP)):\n",
    "        inputs_x,target_x = data\n",
    "        output_mew_pdp = best_mdl(inputs_x,target_x,epoch)\n",
    "        predicts_test.append(output_mew_pdp.flatten())\n",
    "    predicts_test=torch.cat(predicts_test)\n",
    "    predicts_test.numpy()\n",
    "bluesky_data_test['y_pred_t'] = predicts_test\n",
    "bluesky_data_test['y_pred_t_dollar'] = np.exp(bluesky_data_test['y_pred_t'] * trgt_s + trgt_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc10+</th>\n",
       "      <th>median_bias</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp_flag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2424.0</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6376.0</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count   pc10  pc10+  median_bias   mape\n",
       "lp_flag                                          \n",
       "0        2424.0  0.396  0.267       -0.022  0.376\n",
       "1        6376.0  0.689  0.199        0.013  0.106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bluesky_data_test['lp_flag']=np.where((bluesky_data_test.clmls_amt_list_prce.isnull() & bluesky_data_test.ccmls_list_prce.isnull()) ,0, 1)\n",
    "display(bluesky_data_test.groupby(['lp_flag']).apply(lambda d : df_metric_wArg(d,'final_purprice','y_pred_t_dollar')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc10+</th>\n",
       "      <th>median_bias</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6123.0</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count   pc10  pc10+  median_bias   mape\n",
       "0  6123.0  0.622  0.222         0.01  0.165"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hve_prod = pd.read_pickle(f'{datapath}/hve_prod/hve_prod_{time}_statelevel.pkl')\n",
    "hve_day0_cnty = hve_prod[(hve_prod.state==state[0])& (hve_prod.dayflag==0)]\n",
    "df_metric_wArg(bluesky_data_test[bluesky_data_test.final_address.isin(hve_day0_cnty.address.values.tolist())],'final_purprice','y_pred_t_dollar').to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc10+</th>\n",
       "      <th>median_bias</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>452.0</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count   pc10  pc10+  median_bias   mape\n",
       "0  452.0  0.586   0.19       -0.009  0.203"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hve_prod = pd.read_pickle(f'{datapath}/hve_prod/hve_prod_{time}_statelevel.pkl')\n",
    "hve_day0_cnty = hve_prod[(hve_prod.statecounty==\"WA063\")& (hve_prod.dayflag==0)]\n",
    "df_metric_wArg(bluesky_data_test[bluesky_data_test.final_address.isin(hve_day0_cnty.address.values.tolist())],'final_purprice','y_pred_t_dollar').to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc10+</th>\n",
       "      <th>median_bias</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp_flag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2424.0</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6376.0</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count   pc10  pc10+  median_bias   mape\n",
       "lp_flag                                          \n",
       "0        2424.0  0.396  0.267       -0.022  0.376\n",
       "1        6376.0  0.689  0.199        0.013  0.106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bluesky_data_test['lp_flag']=np.where((bluesky_data_test.clmls_amt_list_prce.isnull() & bluesky_data_test.ccmls_list_prce.isnull()) ,0, 1)\n",
    "display(bluesky_data_test.groupby(['lp_flag']).apply(lambda d : df_metric_wArg(d,'final_purprice','y_pred_t_dollar')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluesky_data_test.to_pickle(f'{datapath}/with_pred/{state[0]}_{time}_ple{ple_size}_bluesky_data_test_with_pred_state.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-index",
   "language": "python",
   "name": "geo-index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
